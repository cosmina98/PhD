{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wrapped up MLP regressor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyONGSpXutJvRVGTPneSLOOd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cosmina98/PhD/blob/main/MLP_on_graph_classification_using_PyTorch/Wrapped_up_MLP_regressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw without validation set"
      ],
      "metadata": {
        "id": "Ho8s3IxgyhCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rpUAjz493C9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre --extra-index https://pypi.anaconda.org/scipy-wheels-nightly/simple scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkWYoXHL3DUB",
        "outputId": "ce42c026-4411-4af7-8238-e4f902175acf"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.anaconda.org/scipy-wheels-nightly/simple\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement inspection (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for inspection\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torchmetrics import R2Score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxpNNdvLudwL",
        "outputId": "3d070d5c-a388-4607-a036-669dce610a28"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.9.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.0+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BostonDataset(torch.utils.data.Dataset):\n",
        "  '''\n",
        "  Prepare the Boston dataset for regression\n",
        "  '''\n",
        "\n",
        "  def __init__(self, X, y, scale_data=True):\n",
        "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "      # Apply scaling if necessary\n",
        "      if scale_data:\n",
        "          X = StandardScaler().fit_transform(X)\n",
        "      self.X = torch.from_numpy(X)\n",
        "      self.y = torch.from_numpy(y)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.X)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "      return self.X[i], self.y[i]\n",
        "      \n",
        "\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron for regression.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(13, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "      Forward pass\n",
        "    '''\n",
        "    return self.layers(x)\n",
        "\n",
        "  \n",
        "\n",
        "# Set fixed random number seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load Boston dataset\n",
        "X, y = load_boston(return_X_y=True)\n",
        "# Prepare Boston dataset\n",
        "dataset = BostonDataset(X, y)\n",
        "dataset\n",
        "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "\n",
        "# Initialize the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "# Run the training loop\n",
        "for epoch in range(0, 5): # 5 epochs at maximum\n",
        "  \n",
        "  # Print epoch\n",
        "  print(f'Starting epoch {epoch+1}')\n",
        "  \n",
        "  # Set current loss value\n",
        "  current_loss = 0.0\n",
        "  \n",
        "  # Iterate over the DataLoader for training data\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    \n",
        "    # Get and prepare inputs\n",
        "    inputs, targets = data\n",
        "    inputs, targets = inputs.float(), targets.float()\n",
        "    targets = targets.reshape((targets.shape[0], 1))\n",
        "    \n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Perform forward pass\n",
        "    outputs = mlp(inputs)\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = loss_function(outputs, targets)\n",
        "    \n",
        "    # Perform backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Perform optimization\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Print statistics\n",
        "    current_loss += loss.item()\n",
        "    if i % 10 == 0:\n",
        "        print('Loss after mini-batch %5d: %.3f' %\n",
        "              (i + 1, current_loss / 500))\n",
        "        current_loss = 0.0\n",
        "\n",
        "# Process is complete.\n",
        "print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTgaS8pYymXi",
        "outputId": "a4f263c5-b6b6-43bd-9d6e-8eec0cded6be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after mini-batch     1: 0.043\n",
            "Loss after mini-batch    11: 0.426\n",
            "Loss after mini-batch    21: 0.442\n",
            "Loss after mini-batch    31: 0.461\n",
            "Loss after mini-batch    41: 0.452\n",
            "Loss after mini-batch    51: 0.472\n",
            "Starting epoch 2\n",
            "Loss after mini-batch     1: 0.047\n",
            "Loss after mini-batch    11: 0.432\n",
            "Loss after mini-batch    21: 0.450\n",
            "Loss after mini-batch    31: 0.434\n",
            "Loss after mini-batch    41: 0.485\n",
            "Loss after mini-batch    51: 0.434\n",
            "Starting epoch 3\n",
            "Loss after mini-batch     1: 0.049\n",
            "Loss after mini-batch    11: 0.458\n",
            "Loss after mini-batch    21: 0.435\n",
            "Loss after mini-batch    31: 0.423\n",
            "Loss after mini-batch    41: 0.417\n",
            "Loss after mini-batch    51: 0.480\n",
            "Starting epoch 4\n",
            "Loss after mini-batch     1: 0.051\n",
            "Loss after mini-batch    11: 0.436\n",
            "Loss after mini-batch    21: 0.417\n",
            "Loss after mini-batch    31: 0.452\n",
            "Loss after mini-batch    41: 0.447\n",
            "Loss after mini-batch    51: 0.438\n",
            "Starting epoch 5\n",
            "Loss after mini-batch     1: 0.043\n",
            "Loss after mini-batch    11: 0.442\n",
            "Loss after mini-batch    21: 0.427\n",
            "Loss after mini-batch    31: 0.428\n",
            "Loss after mini-batch    41: 0.424\n",
            "Loss after mini-batch    51: 0.451\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrapped up Pytorch"
      ],
      "metadata": {
        "id": "0TQiS-VaMemq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPClassifer(nn.Module):\n",
        "        '''\n",
        "          Multilayer Perceptron for regression.\n",
        "        '''\n",
        "        def __init__(self):\n",
        "          super().__init__()\n",
        "          hidden_layer_sizes=(64,32)\n",
        "          self.layers = nn.Sequential(\n",
        "            nn.Linear(13, hidden_layer_sizes[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_layer_sizes[1], 1)\n",
        "          )\n",
        "\n",
        "\n",
        "        def forward(self, x):\n",
        "          '''\n",
        "            Forward pass\n",
        "          '''\n",
        "          return self.layers(x)\n",
        "\n",
        "class BostonDataset(torch.utils.data.Dataset):\n",
        "  '''\n",
        "  Prepare the Boston dataset for regression\n",
        "  '''\n",
        "\n",
        "  def __init__(self, X, y, scale_data=True):\n",
        "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "      # Apply scaling if necessary\n",
        "      if scale_data:\n",
        "          X = StandardScaler().fit_transform(X)\n",
        "      self.X = torch.from_numpy(X)\n",
        "      self.y = torch.from_numpy(y)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.X)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "      return self.X[i], self.y[i]\n",
        "\n",
        "class Neural_Network(object):\n",
        "    def __init__ (self,batch_size=10,epochs=2,verbose=True,loss_function = nn.CrossEntropyLoss(),\n",
        "            optimiser='adam',classifier= MLPClassifer(), \n",
        "            learning_rate=1e-3,  betas=(0.9,0.99) , eps=1e-8, weight_decay=0, amsgrad=False,\n",
        "          momentum=0.9, nesterov=True, dampening=0, maximise=False):\n",
        "          \n",
        "          self.batch_size=batch_size\n",
        "          self.epochs=epochs\n",
        "          \n",
        "          self.verbose=True\n",
        "          #used inside the training/ testing loops\n",
        "          self.running_loss = 0\n",
        "          self.loss_function = loss_function\n",
        "          self.classifier= classifier\n",
        "          \n",
        "          self.sm = torch.nn.Softmax()\n",
        "\n",
        "          #some initial parameters for either standard optimisers\n",
        "          self.learning_rate=learning_rate \n",
        "          self.betas= betas\n",
        "          self.eps= eps\n",
        "          self.weight_decay=weight_decay\n",
        "          self.amsgrad=amsgrad \n",
        "          self.momentum=momentum\n",
        "          self.nesterov=nesterov\n",
        "          self.dampening=dampening\n",
        "          self.maximise=maximise\n",
        "\n",
        "            #default optimiser: adam \n",
        "          self.optimiser=self.set_optimiser(optimiser)\n",
        "\n",
        "\n",
        "    def print_self(self):\n",
        "        print(self.classifier)\n",
        "          \n",
        "    def set_optimiser(self,optimiser):\n",
        "        #playing around with adam and sgd only at the moment\n",
        "        #there is only two of them \n",
        "        \n",
        "        if optimiser.lower().strip()=='adam':\n",
        "          self.optimiser=torch.optim.Adam(self.classifier.parameters(),lr=self.learning_rate, betas=self.betas, eps=self.eps, weight_decay=self.weight_decay, amsgrad=self.amsgrad)\n",
        "        elif optimiser.lower().strip()=='sgd':\n",
        "          self.optimiser=torch.optim.SGD(self.classifier.parameters(),lr=self.learning_rate, weight_decay=self.weight_decay, momentum=0.9,nesterov=self.nesterov , dampening=self.dampening)\n",
        "          return self.optimiser\n",
        "\n",
        "\n",
        "\n",
        "    def get_params(self): #get parameters\n",
        "       if (self.optimiser != None) :\n",
        "         return  (self.props(),self.optimiser.state_dict(), self.classifier.state_dict)\n",
        "       else: return self.classifier.state_dict() \n",
        "  \n",
        "    def fit(self,X,y):\n",
        "          # Load Boston dataset\n",
        "        X, y = load_boston(return_X_y=True)\n",
        "        \n",
        "        # Prepare Boston dataset\n",
        "        dataset = BostonDataset(X, y)\n",
        "        train, _ = random_split(dataset,[400, 106])\n",
        "        trainloader = torch.utils.data.DataLoader(train, batch_size=10)\n",
        "        for epoch in range(self.epochs):  # loop over the dataset 2 times\n",
        "\n",
        "          running_loss =self.running_loss\n",
        "          for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, targets = data\n",
        "            inputs, targets = inputs.float(), targets.float()\n",
        "            targets = targets.reshape((targets.shape[0], 1))\n",
        "             \n",
        "            # zero the parameter gradients\n",
        "            self.optimiser.zero_grad()\n",
        "\n",
        "            # forward + compute loss+ backward + optimize\n",
        "            outputs = self.classifier(inputs)\n",
        "            loss = self.loss_function(outputs, targets)\n",
        "            loss.backward()\n",
        "            self.optimiser.step()\n",
        "            \n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % self.batch_size == (self.batch_size-1):    # print every 2000 mini-batches\n",
        "                if self.verbose==True:\n",
        "                  print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 500}')\n",
        "                running_loss = 0.0\n",
        "            \n",
        "\n",
        "\n",
        "    def predict(self,X,y): #Predict using the multi-layer perceptron classifier.\n",
        "      prediction_list=[]\n",
        "      # again no gradients needed\n",
        "      mse_loss_list=[]\n",
        "      valid_loss=[]\n",
        "      dataset = BostonDataset(X, y)\n",
        "      _, test = random_split(dataset,[400, 106])\n",
        "      testloader = torch.utils.data.DataLoader(test, batch_size=10)\n",
        "      with torch.no_grad():\n",
        "          for data in testloader:\n",
        "              inputs, labels = data\n",
        "              inputs, labels = inputs.float(), labels.float()\n",
        "              labels = labels.reshape((labels.shape[0], 1))\n",
        "              outputs = self.classifier(inputs)\n",
        "              loss = self.loss_function(outputs, labels)\n",
        "              valid_loss.append(loss.item())\n",
        "          mse_loss_list.append(np.mean(valid_loss))\n",
        "      return mse_loss_list,self.sm(outputs) #torch.stack(prediction_list), \n",
        "      \n",
        " \n",
        "    def score(self, X,y): \n",
        "      dataset = BostonDataset(X, y)\n",
        "      _, valid = random_split(dataset,[400, 106])\n",
        "      validloader = torch.utils.data.DataLoader(valid, batch_size=10)\n",
        "      r_2_score=[]\n",
        "      with torch.no_grad():\n",
        "        for data in validloader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.float(), labels.float()\n",
        "            labels = labels.reshape((labels.shape[0], 1))\n",
        "            outputs = self.classifier(inputs)\n",
        "            r2score = R2Score()\n",
        "            r_2_score.append(r2score(labels, outputs))\n",
        "      \n",
        "        \n",
        "        return r_2_score\n",
        "\n",
        "    def set_params(self, attr, value): #sets parameters\n",
        "        setattr(self, attr, value)\n",
        "        \n",
        "    def props(cls):   \n",
        "        return [i for i in cls.__dict__.items() if i[:1] != '_']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JK6isQN_OIq0"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1ujfrNmOKGJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set fixed random number seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load Boston dataset\n",
        "X, y = load_boston(return_X_y=True)\n",
        "\n",
        "dataset = BostonDataset(X, y)\n",
        "print(len(dataset))\n",
        "train, valid = random_split(dataset,[400, 106])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train, batch_size=10)\n",
        "validloader = DataLoader(valid, batch_size=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoUPKxmNKoLg",
        "outputId": "b7287d99-ae75-4772-d08b-ee424aa1db6e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Neural_Network()\n",
        "model.set_optimiser('adam')\n",
        "model.set_params('loss_function', nn.L1Loss())\n",
        "model.set_params('verbose', True)\n",
        "model.get_params()\n",
        "model.fit(X,y)\n",
        "model.score(X,y)\n"
      ],
      "metadata": {
        "id": "govYeZQ7urGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fda2f27-a5be-467d-d706-44b0ece09355"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    10] loss: 0.42925367736816405\n",
            "[1,    20] loss: 0.407821662902832\n",
            "[1,    30] loss: 0.476226188659668\n",
            "[1,    40] loss: 0.45705945205688475\n",
            "[2,    10] loss: 0.3899519386291504\n",
            "[2,    20] loss: 0.34760025978088377\n",
            "[2,    30] loss: 0.3960038604736328\n",
            "[2,    40] loss: 0.35091945838928224\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(-27.0535),\n",
              " tensor(-55.8274),\n",
              " tensor(-17.2555),\n",
              " tensor(-46.4803),\n",
              " tensor(-131.6968),\n",
              " tensor(-17.1206),\n",
              " tensor(-24.8929),\n",
              " tensor(-26.7059),\n",
              " tensor(-28.1302),\n",
              " tensor(-104.2911),\n",
              " tensor(-75.5952)]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l2-v4_zat9Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2-3kySLutjdp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}