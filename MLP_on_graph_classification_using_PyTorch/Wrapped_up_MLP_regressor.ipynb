{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wrapped up MLP regressor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXEg6aGU+osYh3RDXSKse0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cosmina98/PhD/blob/main/MLP_on_graph_classification_using_PyTorch/Wrapped_up_MLP_regressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw without validation set"
      ],
      "metadata": {
        "id": "Ho8s3IxgyhCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class BostonDataset(torch.utils.data.Dataset):\n",
        "  '''\n",
        "  Prepare the Boston dataset for regression\n",
        "  '''\n",
        "\n",
        "  def __init__(self, X, y, scale_data=True):\n",
        "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "      # Apply scaling if necessary\n",
        "      if scale_data:\n",
        "          X = StandardScaler().fit_transform(X)\n",
        "      self.X = torch.from_numpy(X)\n",
        "      self.y = torch.from_numpy(y)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.X)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "      return self.X[i], self.y[i]\n",
        "      \n",
        "\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron for regression.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(13, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "      Forward pass\n",
        "    '''\n",
        "    return self.layers(x)\n",
        "\n",
        "  \n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  \n",
        "  # Load Boston dataset\n",
        "  X, y = load_boston(return_X_y=True)\n",
        "  \n",
        "  # Prepare Boston dataset\n",
        "  dataset = BostonDataset(X, y)\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "  \n",
        "  # Initialize the MLP\n",
        "  mlp = MLP()\n",
        "  \n",
        "  # Define the loss function and optimizer\n",
        "  loss_function = nn.L1Loss()\n",
        "  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "  \n",
        "  # Run the training loop\n",
        "  for epoch in range(0, 5): # 5 epochs at maximum\n",
        "    \n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    \n",
        "    # Set current loss value\n",
        "    current_loss = 0.0\n",
        "    \n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      \n",
        "      # Get and prepare inputs\n",
        "      inputs, targets = data\n",
        "      inputs, targets = inputs.float(), targets.float()\n",
        "      targets = targets.reshape((targets.shape[0], 1))\n",
        "      \n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # Perform forward pass\n",
        "      outputs = mlp(inputs)\n",
        "      \n",
        "      # Compute loss\n",
        "      loss = loss_function(outputs, targets)\n",
        "      \n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "      \n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "      \n",
        "      # Print statistics\n",
        "      current_loss += loss.item()\n",
        "      if i % 10 == 0:\n",
        "          print('Loss after mini-batch %5d: %.3f' %\n",
        "                (i + 1, current_loss / 500))\n",
        "          current_loss = 0.0\n",
        "\n",
        "  # Process is complete.\n",
        "  print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTgaS8pYymXi",
        "outputId": "c6e93cc2-7c6c-4ee6-a85f-0529ba540596"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after mini-batch     1: 0.043\n",
            "Loss after mini-batch    11: 0.426\n",
            "Loss after mini-batch    21: 0.442\n",
            "Loss after mini-batch    31: 0.461\n",
            "Loss after mini-batch    41: 0.452\n",
            "Loss after mini-batch    51: 0.472\n",
            "Starting epoch 2\n",
            "Loss after mini-batch     1: 0.047\n",
            "Loss after mini-batch    11: 0.432\n",
            "Loss after mini-batch    21: 0.450\n",
            "Loss after mini-batch    31: 0.434\n",
            "Loss after mini-batch    41: 0.485\n",
            "Loss after mini-batch    51: 0.434\n",
            "Starting epoch 3\n",
            "Loss after mini-batch     1: 0.049\n",
            "Loss after mini-batch    11: 0.458\n",
            "Loss after mini-batch    21: 0.435\n",
            "Loss after mini-batch    31: 0.423\n",
            "Loss after mini-batch    41: 0.417\n",
            "Loss after mini-batch    51: 0.480\n",
            "Starting epoch 4\n",
            "Loss after mini-batch     1: 0.051\n",
            "Loss after mini-batch    11: 0.436\n",
            "Loss after mini-batch    21: 0.417\n",
            "Loss after mini-batch    31: 0.452\n",
            "Loss after mini-batch    41: 0.447\n",
            "Loss after mini-batch    51: 0.438\n",
            "Starting epoch 5\n",
            "Loss after mini-batch     1: 0.043\n",
            "Loss after mini-batch    11: 0.442\n",
            "Loss after mini-batch    21: 0.427\n",
            "Loss after mini-batch    31: 0.428\n",
            "Loss after mini-batch    41: 0.424\n",
            "Loss after mini-batch    51: 0.451\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrapped up Pytorch"
      ],
      "metadata": {
        "id": "0TQiS-VaMemq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "    def __init__ (self):\n",
        "       \n",
        "      self.batch_size=10\n",
        "      self.epochs=5\n",
        "      #used for label count \n",
        "      self.num_classes=0\n",
        "      self.verbose=True\n",
        "        \n",
        "      self.loss_function = None\n",
        "       \n",
        "      #common  to most optimsers\n",
        "      self.learning_rate=1e-4\n",
        "      self.weight_decay=0\n",
        "      self.running_loss = 0.0\n",
        "    \n",
        "      #for sgd  type of optimisers\n",
        "     \n",
        "      #error when trying to set the momentum using self.momentum\n",
        "      self.nesterov= True,\n",
        "      self.dampening=0\n",
        "     \n",
        "      #for adam optimiser\n",
        "      self.betas=(0.9,0.99)\n",
        "      self.eps=1e-8\n",
        "      self.amsgrad=False\n",
        "\n",
        "      self.mlp=self.MLP()\n",
        "      self.optimiser=None\n",
        "      #needed for extraxting the probabilities \n",
        "      self.sm = torch.nn.Softmax()\n",
        "\n",
        "      \n",
        "    class MLP(nn.Module):\n",
        "        '''\n",
        "          Multilayer Perceptron for regression.\n",
        "        '''\n",
        "        def __init__(self):\n",
        "          super().__init__()\n",
        "          self.layers = nn.Sequential(\n",
        "            nn.Linear(13, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "          )\n",
        "\n",
        "\n",
        "        def forward(self, x):\n",
        "          '''\n",
        "            Forward pass\n",
        "          '''\n",
        "          return self.layers(x)\n",
        "\n",
        "\n",
        "    def print_self(self):\n",
        "        print(mlp)\n",
        "          \n",
        "    def set_optimiser(self,optimiser_choice):\n",
        "        #playing around with adam and sgd only at the moment\n",
        "        #there is only two of them \n",
        "        \n",
        "        if optimiser_choice.lower().strip()=='adam':\n",
        "          self.optimiser=torch.optim.Adam(self.mlp.parameters(),lr=self.learning_rate, betas=self.betas, eps=self.eps, weight_decay=self.weight_decay, amsgrad=self.amsgrad)\n",
        "        elif optimiser_choice.lower().strip()=='sgd':\n",
        "          self.optimiser=torch.optim.SGD(self.mlp.parameters(),lr=self.learning_rate, weight_decay=self.weight_decay, momentum=0.9,nesterov=self.nesterov , dampening=self.dampening)\n",
        "          return self.optimiser\n",
        "\n",
        "\n",
        "\n",
        "    def get_params(self): #get parameters\n",
        "       if (self.optimiser != None) :\n",
        "         return  (self.props(),self.optimiser.state_dict(), self.mlp.state_dict)\n",
        "       else: return self.mlp.state_dict() \n",
        "  \n",
        "    def fit(self,trainloader):\n",
        "        for epoch in range(self.epochs):  # loop over the dataset 2 times\n",
        "\n",
        "          running_loss =self.running_loss\n",
        "          for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, targets = data\n",
        "            inputs, targets = inputs.float(), targets.float()\n",
        "            targets = targets.reshape((targets.shape[0], 1))\n",
        "             \n",
        "            # zero the parameter gradients\n",
        "            self.optimiser.zero_grad()\n",
        "\n",
        "            # forward + compute loss+ backward + optimize\n",
        "            outputs = self.mlp(inputs)\n",
        "            loss = self.loss_function(outputs, targets)\n",
        "            loss.backward()\n",
        "            self.optimiser.step()\n",
        "            \n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % self.batch_size == (self.batch_size-1):    # print every 2000 mini-batches\n",
        "                if self.verbose==True:\n",
        "                  print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 500}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "\n",
        "    def predict(self,testloader): #Predict using the multi-layer perceptron classifier.\n",
        "      prediction_list=[]\n",
        "      # again no gradients needed\n",
        "      mse_loss_list=[]\n",
        "      valid_loss=[]\n",
        "      with torch.no_grad():\n",
        "          for data in testloader:\n",
        "              inputs, labels = data\n",
        "              inputs, labels = inputs.float(), labels.float()\n",
        "              labels = labels.reshape((labels.shape[0], 1))\n",
        "              outputs = self.mlp(inputs)\n",
        "              loss = self.loss_function(outputs, labels)\n",
        "              valid_loss.append(loss.item())\n",
        "          mse_loss_list.append(np.mean(valid_loss))\n",
        "      return mse_loss_list #torch.stack(prediction_list)\n",
        "\n",
        " \n",
        "    def score(self, validloader): \n",
        "        \n",
        "        r_2_score=[]\n",
        "        with torch.no_grad():\n",
        "          for data in validloader:\n",
        "              inputs, labels = data\n",
        "              inputs, labels = inputs.float(), labels.float()\n",
        "              labels = labels.reshape((labels.shape[0], 1))\n",
        "              outputs = self.mlp(inputs)\n",
        "              r2score = R2Score()\n",
        "              r_2_score.append(r2score(labels, outputs))\n",
        "        \n",
        "        \n",
        "        return r_2_score\n",
        "\n",
        "\n",
        "\n",
        "    def set_params(self, attr, value): #sets parameters\n",
        "        setattr(self, attr, value)\n",
        "        \n",
        "    def props(cls):   \n",
        "        return [i for i in cls.__dict__.items() if i[:1] != '_']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JK6isQN_OIq0"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#w eneed to install  torchmetrics to get the r2 score\n",
        "pip install torchmetrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6-R7Unji2Qi",
        "outputId": "76d437e0-2462-4b64-9168-c0cbdf3daff6"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.9.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.0+cu113)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "from torchmetrics import R2Score\n",
        "\n",
        "class BostonDataset(torch.utils.data.Dataset):\n",
        "  '''\n",
        "  Prepare the Boston dataset for regression\n",
        "  '''\n",
        "\n",
        "  def __init__(self, X, y, scale_data=True):\n",
        "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "      # Apply scaling if necessary\n",
        "      if scale_data:\n",
        "          X = StandardScaler().fit_transform(X)\n",
        "      self.X = torch.from_numpy(X)\n",
        "      self.y = torch.from_numpy(y)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.X)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "      return self.X[i], self.y[i]"
      ],
      "metadata": {
        "id": "1ujfrNmOKGJX"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  \n",
        "  # Load Boston dataset\n",
        "  X, y = load_boston(return_X_y=True)\n",
        "  \n",
        "  # Prepare Boston dataset\n",
        "  dataset = BostonDataset(X, y)\n",
        "  print(len(dataset))\n",
        "  train, valid = random_split(dataset,[400, 106])\n",
        "\n",
        "  trainloader = torch.utils.data.DataLoader(train, batch_size=10)\n",
        "  validloader = DataLoader(valid, batch_size=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoUPKxmNKoLg",
        "outputId": "6ebde3c3-e217-43a0-8c07-f6d47c014db0"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Trainer()\n",
        "model.set_optimiser('adam')\n",
        "model.set_params('loss_function', nn.L1Loss())\n",
        "model.set_params('verbose', True)\n",
        "model.get_params()\n",
        "model.fit(trainloader)\n",
        "model.score(validloader)\n"
      ],
      "metadata": {
        "id": "govYeZQ7urGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0578da52-e1b5-4f5b-eccd-91d78781fe1d"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    10] loss: 0.47317152786254885\n",
            "[1,    20] loss: 0.42787820053100584\n",
            "[1,    30] loss: 0.4415381317138672\n",
            "[1,    40] loss: 0.43679176712036133\n",
            "[1,    50] loss: 0.47385559844970704\n",
            "[2,    10] loss: 0.4440377197265625\n",
            "[2,    20] loss: 0.4510718574523926\n",
            "[2,    30] loss: 0.44541382598876955\n",
            "[2,    40] loss: 0.43106599044799804\n",
            "[2,    50] loss: 0.4625949592590332\n",
            "[3,    10] loss: 0.4400681800842285\n",
            "[3,    20] loss: 0.49419192123413086\n",
            "[3,    30] loss: 0.4300418128967285\n",
            "[3,    40] loss: 0.4416669006347656\n",
            "[3,    50] loss: 0.4215982246398926\n",
            "[4,    10] loss: 0.4415210647583008\n",
            "[4,    20] loss: 0.4168957633972168\n",
            "[4,    30] loss: 0.43955612564086916\n",
            "[4,    40] loss: 0.4652027587890625\n",
            "[4,    50] loss: 0.44356776809692383\n",
            "[5,    10] loss: 0.4585772590637207\n",
            "[5,    20] loss: 0.40800986099243164\n",
            "[5,    30] loss: 0.4080489158630371\n",
            "[5,    40] loss: 0.4758206481933594\n",
            "[5,    50] loss: 0.42670248794555665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(-inf),\n",
              " tensor(-inf),\n",
              " tensor(-inf),\n",
              " tensor(-inf),\n",
              " tensor(-inf),\n",
              " tensor(-inf),\n",
              " tensor(-inf),\n",
              " tensor(-inf),\n",
              " tensor(-inf),\n",
              " tensor(-inf),\n",
              " tensor(-inf)]"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_params()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ddd872-ab78-4f76-f0e1-9b6aaeb3d24a",
        "id": "ucnJZRtWeJ2i"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([('batch_size', 10),\n",
              "  ('epochs', 5),\n",
              "  ('num_classes', 0),\n",
              "  ('verbose', True),\n",
              "  ('loss_function', L1Loss()),\n",
              "  ('learning_rate', 0.0001),\n",
              "  ('weight_decay', 0),\n",
              "  ('running_loss', 0.0),\n",
              "  ('nesterov', (True,)),\n",
              "  ('dampening', 0),\n",
              "  ('betas', (0.9, 0.99)),\n",
              "  ('eps', 1e-08),\n",
              "  ('amsgrad', False),\n",
              "  ('mlp', MLP(\n",
              "     (layers): Sequential(\n",
              "       (0): Linear(in_features=13, out_features=64, bias=True)\n",
              "       (1): ReLU()\n",
              "       (2): Linear(in_features=64, out_features=32, bias=True)\n",
              "       (3): ReLU()\n",
              "       (4): Linear(in_features=32, out_features=1, bias=True)\n",
              "     )\n",
              "   )),\n",
              "  ('optimiser', Adam (\n",
              "   Parameter Group 0\n",
              "       amsgrad: False\n",
              "       betas: (0.9, 0.99)\n",
              "       capturable: False\n",
              "       eps: 1e-08\n",
              "       foreach: None\n",
              "       lr: 0.0001\n",
              "       maximize: False\n",
              "       weight_decay: 0\n",
              "   )),\n",
              "  ('sm', Softmax(dim=None))],\n",
              " {'param_groups': [{'amsgrad': False,\n",
              "    'betas': (0.9, 0.99),\n",
              "    'capturable': False,\n",
              "    'eps': 1e-08,\n",
              "    'foreach': None,\n",
              "    'lr': 0.0001,\n",
              "    'maximize': False,\n",
              "    'params': [0, 1, 2, 3, 4, 5],\n",
              "    'weight_decay': 0}],\n",
              "  'state': {0: {'exp_avg': tensor([[ 1.3512e-02, -5.3824e-03,  2.0799e-02, -3.4882e-03,  1.8656e-02,\n",
              "             -1.3027e-02,  1.3011e-02, -1.1688e-02,  1.9663e-02,  2.3013e-02,\n",
              "              1.1689e-02, -1.2587e-02,  1.9687e-02],\n",
              "            [-6.5552e-03,  1.1316e-02, -1.5722e-02, -5.0540e-04, -1.6619e-02,\n",
              "              1.7185e-03, -1.6972e-02,  1.6715e-02, -1.6048e-02, -1.5220e-02,\n",
              "             -1.0444e-02,  1.2360e-02, -1.1472e-02],\n",
              "            [-3.3964e-03,  3.2081e-03, -7.2020e-03,  1.7068e-03, -6.5822e-03,\n",
              "              2.3299e-03, -5.0707e-03,  5.4264e-03, -6.5712e-03, -6.5924e-03,\n",
              "             -2.7713e-03,  6.4468e-03, -5.1201e-03],\n",
              "            [-9.3842e-03,  8.8313e-03, -1.7941e-02, -8.5987e-03, -1.8855e-02,\n",
              "              5.5368e-03, -1.4524e-02,  1.4628e-02, -1.8394e-02, -1.8086e-02,\n",
              "             -5.9224e-03,  1.0940e-02, -1.3898e-02],\n",
              "            [ 1.5016e-03, -1.9404e-03,  2.7119e-04, -6.4630e-03, -4.1369e-05,\n",
              "             -3.4198e-03,  1.3714e-03,  2.0933e-04,  2.7912e-03,  3.2882e-03,\n",
              "              5.7268e-03, -7.7349e-04,  3.4318e-03],\n",
              "            [ 8.4600e-03,  9.4322e-03,  1.7494e-03, -1.1579e-02,  1.9388e-03,\n",
              "              2.8530e-03,  1.5674e-03, -1.4375e-03,  9.9521e-03,  1.0183e-02,\n",
              "             -6.1240e-04,  3.0393e-03,  2.3747e-03],\n",
              "            [-8.5197e-03,  3.0999e-03, -1.0612e-02,  4.6688e-03, -1.0358e-02,\n",
              "             -1.4892e-03, -8.1795e-03,  7.7407e-03, -1.7648e-02, -1.5304e-02,\n",
              "             -7.6815e-03,  1.0569e-02, -7.2799e-03],\n",
              "            [ 1.7780e-03,  3.0913e-04,  1.9972e-03, -1.3261e-03,  1.7901e-03,\n",
              "             -2.2853e-03,  2.5343e-04, -1.6766e-04,  2.7109e-03,  3.4221e-03,\n",
              "              4.1257e-03, -1.9848e-03,  3.1449e-03],\n",
              "            [-5.9016e-03, -1.1535e-03, -2.5931e-03,  2.5551e-03, -3.3180e-03,\n",
              "             -2.6876e-03, -3.5275e-03,  2.0335e-03, -4.7994e-03, -4.3224e-03,\n",
              "             -4.8688e-03,  8.0974e-03, -3.3620e-03],\n",
              "            [ 3.2908e-03, -2.1755e-04,  2.5871e-03,  5.8751e-04,  3.9256e-03,\n",
              "             -2.4950e-03,  2.1973e-03, -1.3308e-03,  5.1162e-03,  5.9952e-03,\n",
              "              1.9893e-03, -3.0182e-03,  4.0318e-03],\n",
              "            [ 2.2510e-02, -1.2029e-02,  3.8002e-02,  1.5295e-02,  3.3512e-02,\n",
              "             -1.6854e-02,  2.5563e-02, -2.5488e-02,  3.2819e-02,  3.7878e-02,\n",
              "              1.2747e-02, -2.1113e-02,  3.1109e-02],\n",
              "            [ 1.2907e-03,  8.7449e-04, -4.1707e-04,  5.1720e-04, -5.1543e-04,\n",
              "             -1.8449e-03, -9.2350e-04,  3.3892e-04,  2.0844e-03,  2.4329e-03,\n",
              "              2.2621e-03, -1.0489e-03,  1.4003e-03],\n",
              "            [ 5.9608e-03,  7.8617e-03,  2.7868e-03, -4.2549e-03,  2.9881e-03,\n",
              "             -9.3389e-05,  8.5695e-04, -1.2857e-03,  9.4184e-03,  9.2832e-03,\n",
              "             -1.4728e-03, -8.3074e-03,  3.3023e-03],\n",
              "            [ 1.1420e-02, -6.8280e-03,  1.8372e-02,  1.9525e-03,  1.6410e-02,\n",
              "             -1.1539e-02,  1.0696e-02, -1.1148e-02,  1.6649e-02,  1.9168e-02,\n",
              "              9.5844e-03, -1.0653e-02,  1.6708e-02],\n",
              "            [-1.1933e-02,  1.1376e-02, -1.9498e-02, -2.5739e-03, -2.2084e-02,\n",
              "              7.5416e-03, -1.8348e-02,  1.6793e-02, -2.2781e-02, -2.3298e-02,\n",
              "             -1.3915e-02,  1.5598e-02, -1.6931e-02],\n",
              "            [ 1.0226e-03,  2.6667e-03,  7.1975e-04,  2.6304e-03,  5.4867e-05,\n",
              "              2.5567e-03, -2.3701e-03,  4.7971e-04,  4.0171e-03,  3.7636e-03,\n",
              "              1.8879e-03, -2.5578e-03,  4.4586e-04],\n",
              "            [-6.7532e-03,  8.1642e-04, -1.2563e-02, -1.7819e-03, -1.0110e-02,\n",
              "              1.3578e-03, -5.8539e-03,  6.3547e-03, -1.9747e-02, -1.8371e-02,\n",
              "             -1.1389e-02,  1.1313e-02, -9.2171e-03],\n",
              "            [-9.5016e-03,  1.7195e-02, -1.4373e-02,  6.0120e-03, -1.7741e-02,\n",
              "              1.6033e-02, -7.1081e-03,  1.0805e-02, -2.2923e-02, -2.1774e-02,\n",
              "             -1.0465e-02,  1.6665e-02, -1.5783e-02],\n",
              "            [ 3.7944e-04, -4.2153e-03, -1.7940e-04, -1.1308e-02,  3.3576e-03,\n",
              "              7.1222e-03,  6.0542e-03, -6.4489e-03,  9.3639e-03,  9.8588e-03,\n",
              "             -2.9770e-03, -5.1069e-03, -2.6988e-04],\n",
              "            [ 6.7370e-03, -6.2487e-03,  8.7036e-03,  5.3748e-04,  1.1648e-02,\n",
              "             -8.2947e-03,  1.1519e-02, -9.9322e-03,  1.0476e-02,  1.2685e-02,\n",
              "              5.9350e-03, -5.4413e-03,  1.1684e-02],\n",
              "            [ 4.7013e-03, -2.2984e-03,  5.6777e-03, -1.4501e-03,  7.6802e-03,\n",
              "              4.6744e-04,  5.3886e-03, -5.7765e-03,  8.5732e-03,  8.0146e-03,\n",
              "              3.0335e-03, -9.2791e-03,  4.8051e-03],\n",
              "            [-2.9078e-03,  1.3220e-02, -1.7466e-02,  2.1212e-03, -2.3019e-02,\n",
              "             -1.0356e-02, -1.6032e-02,  2.6895e-02, -1.7002e-02, -1.5390e-02,\n",
              "              3.0970e-03, -1.4669e-02, -4.6869e-04],\n",
              "            [ 1.0184e-03, -9.4107e-04,  1.6926e-03, -2.6667e-03,  1.8959e-03,\n",
              "              2.9604e-04,  7.2950e-04, -9.1045e-04,  1.8095e-03,  2.1365e-03,\n",
              "              4.2602e-04, -1.3248e-03,  2.7140e-04],\n",
              "            [-5.8936e-03,  6.4832e-03, -1.0743e-02, -4.1935e-03, -1.3005e-02,\n",
              "              6.3575e-03, -9.0146e-03,  9.6956e-03, -1.0659e-02, -1.1311e-02,\n",
              "             -6.9338e-03,  7.6578e-03, -1.0358e-02],\n",
              "            [-1.4463e-02,  1.9879e-02, -3.6533e-02, -1.6393e-02, -3.5304e-02,\n",
              "              1.3320e-02, -2.8543e-02,  3.0987e-02, -2.8138e-02, -3.0020e-02,\n",
              "             -8.0805e-03,  1.9829e-02, -2.4985e-02],\n",
              "            [-2.2455e-04, -1.3808e-03,  2.1681e-03,  1.6840e-03,  2.7368e-03,\n",
              "             -1.2527e-03,  3.2445e-03, -1.8817e-03, -2.9875e-05,  8.7672e-04,\n",
              "              1.7689e-05,  4.6051e-03,  1.9110e-03],\n",
              "            [ 1.4526e-02, -2.3887e-03,  1.6674e-02, -6.5956e-03,  1.6397e-02,\n",
              "             -1.0538e-02,  8.1091e-03, -1.0745e-02,  2.2090e-02,  2.5162e-02,\n",
              "              8.4293e-03, -1.1657e-02,  1.5487e-02],\n",
              "            [ 4.5971e-03, -6.3804e-03,  1.1739e-02,  8.5403e-04,  1.1631e-02,\n",
              "             -2.5726e-03,  1.0236e-02, -1.0155e-02,  1.1669e-02,  1.2298e-02,\n",
              "              6.2776e-03, -8.0827e-03,  8.0413e-03],\n",
              "            [-2.1703e-03,  4.7194e-03, -1.0131e-02, -6.8927e-03, -9.9285e-03,\n",
              "              4.2069e-03, -7.6844e-03,  7.3701e-03, -6.7973e-03, -7.0669e-03,\n",
              "             -2.3828e-03, -1.9387e-03, -7.3284e-03],\n",
              "            [-2.1222e-03,  2.1714e-03, -6.7290e-03, -1.0783e-02, -5.9938e-03,\n",
              "              2.9633e-03, -4.8260e-03,  5.0518e-03, -2.0805e-03, -1.5050e-03,\n",
              "              8.6008e-04,  6.1091e-04, -5.7266e-03],\n",
              "            [-4.7108e-04, -1.8339e-03,  1.2315e-03,  7.6447e-04,  7.8298e-04,\n",
              "             -3.5749e-03,  2.2775e-04, -1.2914e-03, -9.2671e-04, -1.8799e-04,\n",
              "              1.0907e-04,  3.4373e-03,  1.1293e-03],\n",
              "            [-4.5455e-03,  7.4770e-03, -8.3145e-03,  1.0988e-04, -1.0645e-02,\n",
              "              7.9511e-04, -1.0261e-02,  1.0216e-02, -1.0585e-02, -1.0156e-02,\n",
              "             -5.5878e-03,  7.9549e-03, -5.6380e-03],\n",
              "            [ 4.4870e-03, -4.2368e-03,  6.8673e-03, -8.1801e-04,  7.5364e-03,\n",
              "             -1.3943e-03,  6.5344e-03, -6.7057e-03,  1.0358e-02,  9.7621e-03,\n",
              "              4.2284e-03, -5.8112e-03,  5.7931e-03],\n",
              "            [ 5.9927e-03, -2.5985e-03,  8.1059e-03,  6.0335e-05,  6.6696e-03,\n",
              "             -4.5348e-03,  6.1567e-03, -4.3934e-03,  9.1064e-03,  1.0034e-02,\n",
              "              6.2023e-03, -5.1362e-03,  8.0019e-03],\n",
              "            [ 1.8675e-02, -1.2763e-03,  1.8662e-02, -2.3608e-03,  1.9085e-02,\n",
              "             -8.2789e-03,  9.0777e-03, -9.8043e-03,  2.8518e-02,  3.0730e-02,\n",
              "              1.2647e-02, -1.5494e-02,  1.7589e-02],\n",
              "            [ 1.4613e-02, -8.6355e-03,  2.5496e-02,  1.4982e-04,  2.1629e-02,\n",
              "             -1.4523e-02,  1.3261e-02, -1.5400e-02,  2.1330e-02,  2.5328e-02,\n",
              "              8.5397e-03, -1.3650e-02,  1.9993e-02],\n",
              "            [-8.9514e-03,  7.0020e-03, -1.4452e-02,  4.4584e-03, -1.6006e-02,\n",
              "             -1.4020e-03, -1.3283e-02,  1.2091e-02, -2.1611e-02, -2.0658e-02,\n",
              "             -1.2447e-02,  1.5999e-02, -1.0870e-02],\n",
              "            [ 1.1841e-03,  5.3208e-03, -7.5978e-03, -2.0566e-03, -8.6550e-03,\n",
              "             -7.7004e-03, -7.1279e-03,  1.2268e-02, -7.6404e-03, -5.8940e-03,\n",
              "              1.2551e-03,  3.2589e-03,  1.2747e-03],\n",
              "            [ 8.2518e-03, -1.9789e-03,  9.0463e-03,  3.7431e-03,  9.6305e-03,\n",
              "             -4.5947e-03,  5.6172e-03, -5.9009e-03,  1.2761e-02,  1.4217e-02,\n",
              "              4.3301e-03, -6.8521e-03,  9.2934e-03],\n",
              "            [ 1.6841e-02, -2.6025e-03,  1.6191e-02, -3.3874e-03,  1.5286e-02,\n",
              "             -8.0594e-03,  8.1042e-03, -9.0479e-03,  2.5393e-02,  2.7036e-02,\n",
              "              1.3225e-02, -1.1239e-02,  1.5469e-02],\n",
              "            [ 6.4248e-03, -4.4251e-03,  1.1226e-02, -3.8092e-04,  1.0501e-02,\n",
              "             -6.2478e-03,  8.2115e-03, -7.0364e-03,  9.3931e-03,  1.0917e-02,\n",
              "              3.0129e-03, -5.8930e-03,  9.9196e-03],\n",
              "            [-2.9752e-04,  7.5804e-03, -1.2227e-03, -3.7971e-03, -2.9477e-03,\n",
              "              3.1703e-03, -4.7819e-03,  5.3986e-03, -2.7522e-03, -2.1264e-03,\n",
              "             -6.4661e-03,  1.2968e-03, -4.3649e-03],\n",
              "            [ 7.4728e-03,  3.9511e-03,  6.2443e-03, -7.2055e-03,  5.4787e-03,\n",
              "             -2.1481e-03,  2.9801e-03, -2.0269e-03,  1.0496e-02,  1.1842e-02,\n",
              "              4.3403e-03, -7.4323e-03,  6.2522e-03],\n",
              "            [ 6.1545e-03, -4.0505e-04,  6.1209e-03, -6.4022e-03,  1.1496e-03,\n",
              "             -7.0022e-03, -2.2787e-03, -1.5801e-03,  5.2498e-03,  6.8848e-03,\n",
              "              6.4702e-03, -6.8311e-03,  7.2335e-03],\n",
              "            [-7.0248e-03,  6.8702e-03, -9.4133e-03,  3.2301e-03, -8.0800e-03,\n",
              "              4.1336e-03, -2.8734e-03,  6.5519e-03, -1.5663e-02, -1.4457e-02,\n",
              "             -1.0504e-02,  1.0390e-02, -6.0979e-03],\n",
              "            [ 6.7657e-03, -3.3645e-03,  8.4018e-03, -8.4805e-03,  4.5461e-03,\n",
              "             -8.5721e-03,  8.5341e-03, -6.3647e-03,  3.5411e-03,  6.7915e-03,\n",
              "              7.4711e-03, -1.0673e-02,  1.2056e-02],\n",
              "            [ 5.0956e-04,  8.8983e-04, -1.0367e-03, -1.7048e-03, -1.0181e-04,\n",
              "              1.4042e-03, -1.0578e-03,  5.8187e-04,  2.0350e-03,  1.8786e-03,\n",
              "             -3.9074e-04, -1.1101e-03, -2.7713e-04],\n",
              "            [ 3.3656e-03,  2.2358e-03,  1.8849e-03, -6.1181e-03,  2.4583e-03,\n",
              "              3.4734e-03,  7.9765e-04, -4.0109e-04,  2.7940e-03,  4.4475e-03,\n",
              "             -2.7766e-03, -8.3555e-03,  8.7731e-04],\n",
              "            [ 5.2052e-04, -8.0691e-04,  4.1801e-04,  5.1553e-04,  6.2150e-04,\n",
              "              1.5186e-03,  1.3289e-03, -9.5880e-04,  5.8311e-04,  1.8139e-04,\n",
              "             -1.0424e-03, -2.6603e-05, -3.4251e-04],\n",
              "            [-6.5471e-05,  1.1355e-02,  7.0688e-04,  6.9728e-03,  1.7793e-03,\n",
              "              1.2331e-02, -8.0276e-03,  5.1239e-03,  5.1440e-03,  2.4231e-03,\n",
              "             -1.0799e-02, -4.7036e-03, -3.0913e-03],\n",
              "            [-1.1922e-02,  6.1909e-03, -1.9494e-02,  4.4393e-03, -1.9191e-02,\n",
              "              5.8507e-03, -1.5717e-02,  1.1570e-02, -2.3303e-02, -2.2162e-02,\n",
              "             -1.6649e-02,  1.5516e-02, -1.4712e-02],\n",
              "            [-1.2630e-02,  1.4544e-02, -2.3536e-02,  4.2518e-03, -2.3475e-02,\n",
              "              7.5508e-03, -2.3133e-02,  1.9287e-02, -3.1687e-02, -3.0607e-02,\n",
              "             -2.4490e-02,  1.5958e-02, -2.1735e-02],\n",
              "            [ 3.1757e-03,  9.6710e-04,  5.5354e-03, -1.0743e-03,  5.5579e-03,\n",
              "             -2.7971e-03,  4.1706e-03, -3.8453e-03,  4.3097e-03,  5.6654e-03,\n",
              "             -1.1110e-03, -3.0023e-03,  5.3236e-03],\n",
              "            [-3.3775e-03,  1.1507e-02, -1.2071e-02,  6.4052e-03, -1.1580e-02,\n",
              "              1.0231e-02, -7.5790e-03,  1.2791e-02, -1.3941e-02, -1.5292e-02,\n",
              "             -5.9534e-03,  1.4671e-02, -1.3557e-02],\n",
              "            [-9.6923e-04, -1.1236e-03, -1.2935e-03, -6.1003e-03, -2.2180e-03,\n",
              "             -3.5772e-03, -2.5791e-03,  3.0624e-03, -2.2788e-03, -1.3377e-03,\n",
              "              1.4213e-03,  1.4017e-03,  8.9260e-05],\n",
              "            [-4.0987e-04,  6.2365e-03, -5.4824e-03,  4.6952e-03, -6.7788e-03,\n",
              "              6.9728e-03, -8.2824e-03,  4.5242e-03, -4.9945e-03, -5.1473e-03,\n",
              "             -5.3383e-03,  3.2084e-03, -6.7556e-03],\n",
              "            [-1.6464e-03,  1.7172e-04, -2.1929e-03, -3.5560e-04, -5.3262e-03,\n",
              "             -7.5823e-05, -4.4812e-03,  4.3192e-03, -3.0756e-03, -2.9983e-03,\n",
              "              2.1466e-03, -1.5561e-03, -2.0398e-03],\n",
              "            [ 6.5545e-03,  5.5805e-03,  8.7114e-05, -5.8837e-03, -1.2438e-03,\n",
              "              8.7475e-03,  5.7816e-04, -1.1661e-03,  1.5253e-03,  1.2879e-03,\n",
              "             -5.5392e-04, -1.2504e-02,  9.2551e-04],\n",
              "            [-1.1567e-03,  2.2712e-03, -4.2905e-03,  1.2545e-03, -3.6955e-03,\n",
              "              4.1865e-04, -3.3913e-03,  3.4735e-03, -3.7501e-03, -3.4631e-03,\n",
              "             -8.5223e-04,  3.2195e-03, -2.3778e-03],\n",
              "            [ 1.4384e-02, -5.8152e-03,  2.0767e-02, -1.1974e-02,  1.3536e-02,\n",
              "             -1.3683e-02,  1.0159e-02, -1.1983e-02,  2.0672e-02,  2.4860e-02,\n",
              "              1.4938e-02, -1.2516e-02,  2.0719e-02],\n",
              "            [ 3.3310e-03, -4.2418e-03,  7.1125e-03,  1.0788e-03,  5.0655e-03,\n",
              "             -5.4182e-03,  2.6234e-03, -4.7339e-03,  4.7667e-03,  6.2054e-03,\n",
              "              1.5745e-03, -1.7006e-03,  5.6011e-03],\n",
              "            [-2.6672e-03,  7.9637e-03, -6.3992e-03, -1.6508e-04, -6.8404e-03,\n",
              "              3.0951e-03, -7.5585e-03,  8.3735e-03, -3.8796e-03, -5.0406e-03,\n",
              "             -2.5258e-03,  2.4799e-03, -5.2633e-03],\n",
              "            [-5.8467e-03,  3.0550e-03, -4.5055e-03, -1.7611e-02, -5.7611e-03,\n",
              "              3.2351e-03, -5.1854e-03,  5.4749e-03, -3.1883e-03, -2.6004e-03,\n",
              "              6.5754e-04,  1.1643e-03, -5.1834e-03],\n",
              "            [-4.5429e-03,  1.0205e-02, -1.3694e-02, -1.1527e-02, -1.2794e-02,\n",
              "              1.7129e-03, -6.2216e-03,  1.0271e-02, -1.8114e-02, -1.7011e-02,\n",
              "             -5.0380e-03, -2.8182e-03, -7.6186e-03]]),\n",
              "    'exp_avg_sq': tensor([[1.1204e-04, 1.2580e-04, 3.2260e-04, 1.8051e-04, 2.6183e-04, 1.7317e-04,\n",
              "             2.1218e-04, 2.0025e-04, 2.5509e-04, 3.4055e-04, 2.1765e-04, 9.4683e-05,\n",
              "             2.6000e-04],\n",
              "            [5.6483e-05, 6.2570e-05, 1.6456e-04, 3.7116e-05, 1.3333e-04, 4.5507e-05,\n",
              "             1.4216e-04, 1.2827e-04, 1.8744e-04, 1.7891e-04, 1.1842e-04, 2.0472e-04,\n",
              "             1.2870e-04],\n",
              "            [2.3322e-05, 5.7905e-06, 3.2649e-05, 2.2830e-05, 2.8861e-05, 1.3420e-05,\n",
              "             1.6550e-05, 1.6271e-05, 3.4175e-05, 3.5057e-05, 1.3460e-05, 7.3327e-05,\n",
              "             3.4190e-05],\n",
              "            [1.5279e-04, 5.2810e-05, 2.5067e-04, 1.5783e-04, 2.8327e-04, 8.9022e-05,\n",
              "             1.5935e-04, 1.6258e-04, 2.8900e-04, 2.8581e-04, 7.7133e-05, 2.0612e-04,\n",
              "             2.4940e-04],\n",
              "            [3.7309e-06, 1.6716e-05, 8.3115e-06, 4.2081e-05, 1.2937e-05, 1.5159e-05,\n",
              "             1.1770e-05, 8.1242e-06, 9.6391e-06, 9.9707e-06, 3.3720e-05, 1.0887e-05,\n",
              "             1.4142e-05],\n",
              "            [5.1427e-05, 7.1239e-05, 7.5435e-05, 2.4629e-04, 8.6752e-05, 6.4994e-05,\n",
              "             6.2510e-05, 8.9752e-05, 9.5997e-05, 9.8691e-05, 8.3751e-05, 1.2090e-04,\n",
              "             8.6843e-05],\n",
              "            [7.2237e-05, 1.6093e-05, 7.6117e-05, 1.2140e-05, 8.4103e-05, 2.2334e-05,\n",
              "             5.9196e-05, 4.9285e-05, 2.1327e-04, 1.6929e-04, 4.6369e-05, 1.3208e-04,\n",
              "             6.1476e-05],\n",
              "            [5.6159e-06, 3.5640e-06, 1.1707e-05, 6.7807e-06, 9.0840e-06, 1.2260e-05,\n",
              "             1.2677e-05, 6.2874e-06, 1.3494e-05, 1.5276e-05, 2.3698e-05, 3.4983e-05,\n",
              "             2.1678e-05],\n",
              "            [7.7037e-05, 1.5734e-05, 2.4407e-05, 4.3862e-06, 2.8657e-05, 1.5209e-05,\n",
              "             2.3212e-05, 2.3133e-05, 4.7287e-05, 3.9697e-05, 2.7258e-05, 1.5521e-04,\n",
              "             3.7702e-05],\n",
              "            [3.9750e-06, 1.4255e-05, 1.3837e-05, 7.3362e-06, 1.1165e-05, 7.3356e-06,\n",
              "             1.5439e-05, 1.6171e-05, 9.7206e-06, 1.3423e-05, 9.0374e-06, 3.3967e-06,\n",
              "             1.0001e-05],\n",
              "            [3.5376e-04, 6.4848e-04, 1.2245e-03, 1.8212e-04, 1.0189e-03, 4.1630e-04,\n",
              "             8.8447e-04, 9.8438e-04, 7.7829e-04, 1.0387e-03, 5.0489e-04, 2.9988e-04,\n",
              "             7.3727e-04],\n",
              "            [8.9402e-07, 9.6006e-07, 3.9024e-06, 1.6483e-06, 2.7754e-06, 5.0944e-06,\n",
              "             3.5022e-06, 3.6432e-06, 2.6845e-06, 3.2892e-06, 8.0860e-06, 9.4037e-07,\n",
              "             2.9222e-06],\n",
              "            [3.2306e-05, 4.0337e-05, 6.3175e-05, 5.3792e-05, 5.4216e-05, 3.4921e-05,\n",
              "             5.6785e-05, 5.4392e-05, 7.1800e-05, 7.8674e-05, 4.5257e-05, 4.0893e-05,\n",
              "             5.5243e-05],\n",
              "            [7.7488e-05, 2.0734e-04, 2.5521e-04, 3.7117e-05, 2.1710e-04, 1.4495e-04,\n",
              "             1.7710e-04, 2.0391e-04, 1.7734e-04, 2.2248e-04, 1.5074e-04, 6.5915e-05,\n",
              "             2.0277e-04],\n",
              "            [1.4107e-04, 6.7552e-05, 2.3229e-04, 4.6780e-05, 2.4446e-04, 8.4173e-05,\n",
              "             1.6963e-04, 1.3958e-04, 2.7712e-04, 2.8654e-04, 1.3830e-04, 2.3770e-04,\n",
              "             2.2094e-04],\n",
              "            [1.5724e-05, 1.1957e-05, 1.8824e-05, 6.1824e-06, 1.2552e-05, 1.0000e-05,\n",
              "             1.2352e-05, 1.9760e-05, 1.9194e-05, 1.8945e-05, 2.5723e-05, 1.1392e-05,\n",
              "             1.2006e-05],\n",
              "            [4.9254e-05, 3.4221e-05, 9.7316e-05, 6.3211e-05, 8.3495e-05, 4.5100e-05,\n",
              "             5.6467e-05, 5.9730e-05, 2.2675e-04, 2.0093e-04, 7.0512e-05, 1.6014e-04,\n",
              "             9.0640e-05],\n",
              "            [2.7704e-04, 2.3235e-04, 2.7111e-04, 1.2392e-04, 3.8226e-04, 3.0684e-04,\n",
              "             2.8200e-04, 2.8606e-04, 5.2780e-04, 4.8316e-04, 2.2577e-04, 5.9462e-04,\n",
              "             4.4100e-04],\n",
              "            [1.6156e-04, 1.7373e-04, 1.0814e-04, 2.8462e-04, 1.2148e-04, 1.4262e-04,\n",
              "             1.4747e-04, 1.9621e-04, 1.0292e-04, 1.1667e-04, 6.4200e-05, 7.1396e-05,\n",
              "             1.5665e-04],\n",
              "            [2.6310e-05, 7.1362e-05, 8.0031e-05, 3.9765e-05, 9.7191e-05, 6.2053e-05,\n",
              "             1.1563e-04, 1.0414e-04, 6.5652e-05, 8.5451e-05, 5.7724e-05, 1.9223e-05,\n",
              "             8.6078e-05],\n",
              "            [4.0558e-05, 1.0626e-05, 3.1892e-05, 7.4523e-06, 6.1185e-05, 1.3802e-05,\n",
              "             2.9937e-05, 2.9092e-05, 7.6176e-05, 6.4397e-05, 2.1350e-05, 1.1670e-04,\n",
              "             3.6807e-05],\n",
              "            [1.3570e-04, 2.7370e-04, 3.3999e-04, 2.4619e-04, 5.0509e-04, 2.5614e-04,\n",
              "             4.7085e-04, 5.0831e-04, 5.4737e-04, 4.9597e-04, 4.0760e-04, 1.8037e-04,\n",
              "             2.5194e-04],\n",
              "            [6.6090e-06, 5.7214e-06, 5.9569e-06, 1.4853e-05, 5.0889e-06, 7.8165e-06,\n",
              "             5.1311e-06, 6.5545e-06, 7.2078e-06, 8.1433e-06, 3.4737e-06, 5.0600e-06,\n",
              "             1.1287e-05],\n",
              "            [5.5042e-05, 2.3069e-05, 6.7477e-05, 5.4428e-05, 9.3086e-05, 5.1607e-05,\n",
              "             4.9995e-05, 5.0000e-05, 7.1779e-05, 7.5261e-05, 3.9660e-05, 1.0227e-04,\n",
              "             1.1762e-04],\n",
              "            [3.6150e-04, 2.2597e-04, 9.3736e-04, 6.6527e-04, 8.8167e-04, 3.0206e-04,\n",
              "             5.0030e-04, 5.9365e-04, 5.9530e-04, 6.8295e-04, 2.5549e-04, 5.1475e-04,\n",
              "             6.1189e-04],\n",
              "            [1.4856e-05, 1.4735e-05, 1.2154e-05, 5.0917e-06, 1.0818e-05, 1.1771e-05,\n",
              "             1.1426e-05, 1.1789e-05, 8.8932e-06, 1.0170e-05, 8.1703e-06, 3.5601e-05,\n",
              "             9.2607e-06],\n",
              "            [1.1592e-04, 1.3987e-04, 2.5318e-04, 2.2169e-04, 2.2554e-04, 1.6254e-04,\n",
              "             1.4215e-04, 2.1828e-04, 2.7927e-04, 3.5440e-04, 1.3134e-04, 7.7856e-05,\n",
              "             2.0311e-04],\n",
              "            [4.0463e-05, 3.5505e-05, 1.3368e-04, 2.9715e-05, 1.3196e-04, 2.8951e-05,\n",
              "             8.9115e-05, 8.4512e-05, 1.4052e-04, 1.4961e-04, 5.9180e-05, 1.7949e-04,\n",
              "             8.9014e-05],\n",
              "            [1.4138e-05, 1.6830e-05, 7.9992e-05, 8.7295e-05, 7.2159e-05, 5.3015e-05,\n",
              "             4.2818e-05, 4.2594e-05, 5.7325e-05, 6.3420e-05, 2.4695e-05, 7.4636e-06,\n",
              "             9.5921e-05],\n",
              "            [2.2378e-05, 1.4866e-05, 4.7534e-05, 1.2919e-04, 4.6319e-05, 3.2044e-05,\n",
              "             2.2906e-05, 2.7418e-05, 1.7747e-05, 2.0996e-05, 1.4008e-05, 1.8215e-05,\n",
              "             5.7919e-05],\n",
              "            [8.3304e-06, 1.1111e-05, 7.3242e-06, 1.3205e-06, 5.6047e-06, 8.9727e-06,\n",
              "             4.0041e-06, 1.0796e-05, 5.8796e-06, 5.5295e-06, 4.9517e-06, 1.8850e-05,\n",
              "             5.5553e-06],\n",
              "            [2.8721e-05, 2.4439e-05, 4.0978e-05, 2.5216e-05, 5.5793e-05, 1.3781e-05,\n",
              "             4.7456e-05, 4.3555e-05, 6.7612e-05, 6.1128e-05, 3.4435e-05, 6.8627e-05,\n",
              "             2.5880e-05],\n",
              "            [6.3359e-05, 4.3074e-05, 8.7840e-05, 3.9447e-05, 1.0505e-04, 3.2054e-05,\n",
              "             8.7424e-05, 9.1107e-05, 1.6638e-04, 1.5837e-04, 5.0943e-05, 1.1220e-04,\n",
              "             7.4784e-05],\n",
              "            [2.5030e-05, 4.5582e-05, 6.0220e-05, 2.6952e-05, 5.5326e-05, 3.9291e-05,\n",
              "             5.9422e-05, 4.4486e-05, 6.1744e-05, 7.5348e-05, 5.4122e-05, 1.9284e-05,\n",
              "             5.7532e-05],\n",
              "            [2.1610e-04, 1.5910e-04, 3.4133e-04, 2.2694e-04, 3.2159e-04, 1.6929e-04,\n",
              "             1.6437e-04, 2.5231e-04, 5.2881e-04, 5.4585e-04, 3.0019e-04, 1.5291e-04,\n",
              "             2.6386e-04],\n",
              "            [1.2320e-04, 2.6683e-04, 4.7973e-04, 1.1586e-04, 3.3837e-04, 2.3756e-04,\n",
              "             2.0926e-04, 3.1679e-04, 2.7507e-04, 3.8897e-04, 1.4884e-04, 1.0385e-04,\n",
              "             2.6859e-04],\n",
              "            [6.9596e-05, 3.8053e-05, 1.4387e-04, 1.2448e-05, 1.8286e-04, 3.3809e-05,\n",
              "             1.1157e-04, 9.5647e-05, 2.7562e-04, 2.5719e-04, 1.0082e-04, 3.2394e-04,\n",
              "             1.2064e-04],\n",
              "            [1.4812e-05, 6.1465e-05, 5.6427e-05, 8.4321e-05, 8.9429e-05, 8.7485e-05,\n",
              "             6.7253e-05, 8.8392e-05, 6.6686e-05, 5.5276e-05, 9.8408e-05, 5.9202e-05,\n",
              "             5.2069e-05],\n",
              "            [4.0193e-05, 5.3297e-05, 9.2029e-05, 2.4219e-05, 8.0182e-05, 3.8092e-05,\n",
              "             5.7055e-05, 7.8266e-05, 9.7486e-05, 1.1478e-04, 6.1167e-05, 3.0014e-05,\n",
              "             6.3255e-05],\n",
              "            [1.5836e-04, 1.4328e-04, 2.6931e-04, 1.3312e-04, 2.3597e-04, 1.3221e-04,\n",
              "             1.4254e-04, 2.1523e-04, 3.8429e-04, 3.9884e-04, 2.5452e-04, 1.1022e-04,\n",
              "             1.9635e-04],\n",
              "            [2.0710e-05, 5.8214e-05, 7.9468e-05, 2.1330e-05, 6.7011e-05, 3.3198e-05,\n",
              "             5.4695e-05, 5.4894e-05, 4.6005e-05, 6.0598e-05, 2.6976e-05, 1.6853e-05,\n",
              "             5.1159e-05],\n",
              "            [2.1137e-05, 2.7906e-05, 1.7344e-05, 4.7013e-05, 2.0422e-05, 3.6512e-05,\n",
              "             2.5863e-05, 2.6572e-05, 2.9513e-05, 2.6510e-05, 3.9106e-05, 5.1215e-05,\n",
              "             6.8319e-05],\n",
              "            [2.7638e-05, 2.3617e-05, 5.3339e-05, 9.4116e-05, 4.6385e-05, 4.1028e-05,\n",
              "             3.6115e-05, 4.0853e-05, 6.3616e-05, 7.7157e-05, 4.8465e-05, 2.9004e-05,\n",
              "             5.1711e-05],\n",
              "            [2.5646e-05, 3.5057e-05, 5.3348e-05, 1.4764e-04, 4.0502e-05, 5.8999e-05,\n",
              "             4.0586e-05, 4.7250e-05, 4.8216e-05, 5.4813e-05, 8.5654e-05, 3.0103e-05,\n",
              "             5.2625e-05],\n",
              "            [9.1599e-05, 2.8996e-05, 6.7860e-05, 1.6277e-05, 7.3384e-05, 3.1645e-05,\n",
              "             6.4633e-05, 4.5492e-05, 1.8517e-04, 1.6155e-04, 6.8557e-05, 1.3468e-04,\n",
              "             6.7312e-05],\n",
              "            [4.5965e-05, 8.9153e-05, 9.1245e-05, 1.3771e-04, 9.2551e-05, 6.6552e-05,\n",
              "             1.3593e-04, 1.1463e-04, 7.0445e-05, 8.2304e-05, 7.3749e-05, 5.7176e-05,\n",
              "             9.6907e-05],\n",
              "            [1.3766e-05, 1.4526e-06, 7.0162e-06, 1.5209e-05, 8.4523e-06, 4.3327e-06,\n",
              "             4.5392e-06, 4.6436e-06, 2.0450e-05, 1.8050e-05, 5.2457e-06, 3.8150e-05,\n",
              "             1.3104e-05],\n",
              "            [1.8603e-05, 2.6389e-05, 2.8705e-05, 9.9666e-05, 3.2101e-05, 3.4607e-05,\n",
              "             3.1381e-05, 4.2812e-05, 3.4618e-05, 3.9157e-05, 3.5850e-05, 2.8632e-05,\n",
              "             3.4087e-05],\n",
              "            [1.1639e-05, 6.1833e-06, 1.1758e-05, 3.0155e-05, 6.9446e-06, 2.6832e-05,\n",
              "             1.0406e-05, 8.7426e-06, 1.1320e-05, 1.2016e-05, 2.4812e-05, 5.1664e-06,\n",
              "             1.1151e-05],\n",
              "            [1.4795e-04, 8.4355e-05, 8.5186e-05, 3.0512e-05, 4.9216e-05, 1.6796e-04,\n",
              "             9.2232e-05, 8.6550e-05, 7.1447e-05, 7.4634e-05, 1.7450e-04, 6.3294e-05,\n",
              "             1.4456e-04],\n",
              "            [2.1590e-04, 6.4951e-05, 2.8754e-04, 5.4718e-05, 3.0509e-04, 1.2733e-04,\n",
              "             1.9216e-04, 1.5892e-04, 3.9114e-04, 3.7327e-04, 1.9758e-04, 3.1658e-04,\n",
              "             2.7660e-04],\n",
              "            [2.5220e-04, 1.4201e-04, 4.4952e-04, 7.1373e-05, 4.0228e-04, 1.5093e-04,\n",
              "             4.0099e-04, 2.8040e-04, 7.9889e-04, 7.5388e-04, 4.5117e-04, 4.6840e-04,\n",
              "             4.8016e-04],\n",
              "            [4.2148e-06, 3.8336e-06, 1.5138e-05, 1.2377e-05, 1.3764e-05, 9.3106e-06,\n",
              "             1.4096e-05, 1.3098e-05, 9.4055e-06, 1.5186e-05, 4.1987e-06, 3.9881e-06,\n",
              "             1.3600e-05],\n",
              "            [5.6927e-05, 7.5008e-05, 1.4514e-04, 2.6378e-05, 1.5223e-04, 1.4169e-04,\n",
              "             8.9206e-05, 1.1829e-04, 1.7634e-04, 2.0930e-04, 8.2681e-05, 2.0866e-04,\n",
              "             2.3296e-04],\n",
              "            [1.0478e-05, 1.4962e-05, 6.5715e-06, 4.4700e-05, 6.9272e-06, 1.1786e-05,\n",
              "             7.1863e-06, 6.4665e-06, 8.0197e-06, 6.9713e-06, 9.8126e-06, 6.7967e-06,\n",
              "             6.6093e-06],\n",
              "            [2.5069e-05, 3.9054e-05, 4.6494e-05, 2.8751e-05, 5.9348e-05, 5.3951e-05,\n",
              "             6.3283e-05, 6.0962e-05, 4.1581e-05, 3.5473e-05, 6.4115e-05, 6.8840e-05,\n",
              "             7.5089e-05],\n",
              "            [1.2866e-05, 9.5847e-06, 9.8742e-06, 6.6321e-06, 1.9476e-05, 1.3531e-05,\n",
              "             1.5171e-05, 1.3378e-05, 1.2529e-05, 1.1277e-05, 1.4243e-05, 3.5950e-06,\n",
              "             2.0538e-05],\n",
              "            [5.0692e-05, 1.0485e-04, 1.0152e-04, 1.6438e-04, 1.3604e-04, 1.0546e-04,\n",
              "             1.3271e-04, 1.7232e-04, 1.3664e-04, 1.2106e-04, 1.4861e-04, 8.9338e-05,\n",
              "             1.1310e-04],\n",
              "            [2.2083e-06, 5.2122e-06, 1.7465e-05, 2.4847e-06, 1.1312e-05, 3.3584e-06,\n",
              "             9.8781e-06, 9.3681e-06, 1.0608e-05, 1.0840e-05, 5.3432e-06, 1.5151e-05,\n",
              "             8.4235e-06],\n",
              "            [1.4459e-04, 2.9645e-04, 4.6015e-04, 3.9459e-04, 3.0737e-04, 2.8757e-04,\n",
              "             2.6473e-04, 3.3749e-04, 3.4280e-04, 4.5440e-04, 3.6428e-04, 1.0728e-04,\n",
              "             3.8324e-04],\n",
              "            [1.1669e-05, 3.2315e-05, 4.4791e-05, 1.0999e-05, 2.9401e-05, 2.1862e-05,\n",
              "             2.0718e-05, 3.2067e-05, 2.5376e-05, 3.4050e-05, 1.5340e-05, 8.9019e-06,\n",
              "             2.6212e-05],\n",
              "            [2.4188e-05, 1.4299e-04, 1.0064e-04, 2.3179e-05, 1.0610e-04, 3.4753e-05,\n",
              "             1.3781e-04, 1.7579e-04, 3.9931e-05, 6.4812e-05, 4.2887e-05, 1.4856e-05,\n",
              "             5.5922e-05],\n",
              "            [1.1644e-04, 1.1603e-05, 3.5693e-05, 3.1823e-04, 3.5112e-05, 5.0081e-05,\n",
              "             3.2257e-05, 3.2372e-05, 4.1108e-05, 3.6983e-05, 2.5109e-05, 2.4860e-05,\n",
              "             7.9680e-05],\n",
              "            [3.8485e-05, 5.3573e-05, 1.4894e-04, 1.5936e-04, 1.2459e-04, 6.2569e-05,\n",
              "             9.3435e-05, 7.6465e-05, 2.7494e-04, 2.5067e-04, 7.2247e-05, 4.8489e-05,\n",
              "             1.0648e-04]]),\n",
              "    'step': tensor(200.)},\n",
              "   1: {'exp_avg': tensor([-0.0344, -0.0266, -0.0066, -0.0182, -0.0070, -0.0337, -0.0171, -0.0038,\n",
              "            -0.0094, -0.0082, -0.0569, -0.0036, -0.0237, -0.0290, -0.0233, -0.0133,\n",
              "            -0.0216, -0.0537, -0.0278, -0.0170,  0.0103, -0.0585, -0.0015, -0.0133,\n",
              "            -0.0414, -0.0062, -0.0381,  0.0142, -0.0097, -0.0087, -0.0043, -0.0180,\n",
              "             0.0092, -0.0157, -0.0489, -0.0370, -0.0175, -0.0329, -0.0214, -0.0461,\n",
              "            -0.0162, -0.0185, -0.0216, -0.0235, -0.0150, -0.0309, -0.0018, -0.0257,\n",
              "             0.0026, -0.0256, -0.0281, -0.0320, -0.0085, -0.0246, -0.0078, -0.0269,\n",
              "            -0.0077, -0.0414, -0.0050, -0.0395, -0.0092,  0.0066, -0.0081, -0.0268]),\n",
              "    'exp_avg_sq': tensor([7.3176e-04, 3.2307e-04, 2.5285e-05, 2.4101e-04, 2.2722e-05, 6.0423e-04,\n",
              "            1.6337e-04, 1.6271e-05, 5.9026e-05, 2.4708e-05, 2.2662e-03, 6.9376e-06,\n",
              "            3.1689e-04, 4.9973e-04, 2.8398e-04, 7.6023e-05, 2.3930e-04, 1.7243e-03,\n",
              "            5.3878e-04, 1.6977e-04, 9.7409e-05, 2.2997e-03, 5.2357e-06, 9.8260e-05,\n",
              "            1.0845e-03, 2.1643e-05, 8.1606e-04, 1.7117e-04, 7.0812e-05, 4.5121e-05,\n",
              "            9.8713e-06, 1.3155e-04, 2.2261e-04, 1.6873e-04, 1.5168e-03, 8.0100e-04,\n",
              "            1.5377e-04, 5.0929e-04, 2.7025e-04, 1.1631e-03, 1.3155e-04, 1.5985e-04,\n",
              "            2.4440e-04, 3.2820e-04, 1.3403e-04, 4.8768e-04, 5.9211e-06, 2.8220e-04,\n",
              "            4.6289e-05, 4.1061e-04, 5.0844e-04, 7.1007e-04, 3.0708e-05, 3.3624e-04,\n",
              "            2.5196e-05, 3.5426e-04, 2.8591e-05, 9.7991e-04, 1.6506e-05, 1.0587e-03,\n",
              "            5.5916e-05, 1.0683e-04, 5.6280e-05, 3.4786e-04]),\n",
              "    'step': tensor(200.)},\n",
              "   2: {'exp_avg': tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "              0.0000e+00,  0.0000e+00],\n",
              "            [-6.3935e-02, -6.6543e-02, -3.5713e-02,  ..., -8.5152e-03,\n",
              "             -1.1600e-02, -2.1201e-02],\n",
              "            [ 3.1969e-03,  2.6071e-02,  1.2191e-02,  ...,  4.2832e-04,\n",
              "              8.3200e-03,  1.2406e-02],\n",
              "            ...,\n",
              "            [ 5.6404e-07,  2.6937e-05,  8.0284e-12,  ...,  0.0000e+00,\n",
              "              0.0000e+00,  0.0000e+00],\n",
              "            [-1.5517e-02, -1.3946e-02, -7.2697e-03,  ..., -1.1832e-03,\n",
              "             -2.8784e-03, -5.6347e-03],\n",
              "            [-3.7185e-02, -3.4131e-02, -1.7036e-02,  ..., -4.9465e-03,\n",
              "             -4.4172e-03, -1.1369e-02]]),\n",
              "    'exp_avg_sq': tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "             0.0000e+00],\n",
              "            [3.4192e-03, 3.0608e-03, 9.2082e-04,  ..., 1.3212e-04, 1.4623e-04,\n",
              "             3.8773e-04],\n",
              "            [6.1626e-05, 1.2633e-03, 3.5074e-04,  ..., 6.4606e-06, 1.2682e-04,\n",
              "             2.3603e-04],\n",
              "            ...,\n",
              "            [1.4121e-08, 1.1227e-06, 6.1167e-08,  ..., 0.0000e+00, 0.0000e+00,\n",
              "             0.0000e+00],\n",
              "            [1.0852e-04, 7.4539e-05, 2.8164e-05,  ..., 8.0365e-07, 6.3129e-06,\n",
              "             1.6076e-05],\n",
              "            [1.0495e-03, 6.1520e-04, 1.4078e-04,  ..., 4.2788e-05, 1.9637e-05,\n",
              "             8.4441e-05]]),\n",
              "    'step': tensor(200.)},\n",
              "   3: {'exp_avg': tensor([ 0.0000e+00, -1.2892e-01,  2.1542e-02, -5.1449e-02, -8.9968e-02,\n",
              "            -1.0431e-01, -6.5116e-02, -3.5147e-02,  8.9389e-04,  3.7458e-03,\n",
              "             3.4160e-03, -1.6732e-01,  5.2371e-05, -3.0331e-04,  9.0508e-03,\n",
              "            -2.5438e-02,  8.3194e-03,  6.2702e-03, -4.3501e-02,  6.9120e-05,\n",
              "             2.7605e-04,  2.3375e-02, -1.3267e-01,  4.0209e-03, -3.3738e-02,\n",
              "            -1.0961e-01,  3.8239e-03, -1.3234e-01, -1.6512e-01,  6.8434e-05,\n",
              "            -2.9646e-02, -7.1168e-02]),\n",
              "    'exp_avg_sq': tensor([0.0000e+00, 1.2820e-02, 1.2457e-03, 1.3516e-03, 4.6489e-03, 8.2815e-03,\n",
              "            2.0976e-03, 6.8116e-04, 3.9346e-05, 5.9209e-05, 2.9994e-04, 2.2243e-02,\n",
              "            7.4227e-06, 8.2972e-08, 2.8553e-04, 4.0504e-04, 2.3970e-04, 5.0817e-04,\n",
              "            1.1798e-03, 5.0378e-06, 3.4119e-05, 1.3857e-03, 1.2497e-02, 1.0026e-04,\n",
              "            7.1316e-04, 8.9520e-03, 2.6972e-04, 1.1256e-02, 2.1439e-02, 6.4509e-06,\n",
              "            3.4922e-04, 3.1514e-03]),\n",
              "    'step': tensor(200.)},\n",
              "   4: {'exp_avg': tensor([[ 0.0000e+00, -6.7812e-01, -1.3218e-02, -6.6110e-02, -2.8816e-01,\n",
              "             -3.1230e-01, -2.1557e-01, -5.0479e-01, -3.7429e-04, -3.2221e-03,\n",
              "             -1.0692e-03, -7.2345e-01, -4.1265e-05, -1.2406e-03, -2.1075e-03,\n",
              "             -1.7379e-01, -9.6020e-03, -2.7098e-03, -5.9104e-01, -7.4049e-06,\n",
              "             -1.9862e-01, -1.7648e-02, -5.4099e-01, -1.8077e-03, -2.6348e-01,\n",
              "             -5.2167e-01, -2.2722e-03, -3.7405e-01, -7.1203e-01, -4.2996e-05,\n",
              "             -1.6857e-01, -3.9622e-01]]),\n",
              "    'exp_avg_sq': tensor([[0.0000e+00, 2.7124e-01, 1.2265e-03, 1.6166e-03, 3.8889e-02, 5.3723e-02,\n",
              "             1.9346e-02, 1.2703e-01, 3.4725e-05, 1.2523e-04, 9.0246e-05, 3.1321e-01,\n",
              "             2.3946e-06, 3.0186e-06, 5.7881e-05, 1.4752e-02, 9.2235e-04, 2.8701e-04,\n",
              "             1.9772e-01, 6.3577e-08, 4.8849e-02, 1.2857e-03, 1.6952e-01, 3.5468e-05,\n",
              "             3.8542e-02, 1.4722e-01, 1.1068e-04, 6.8432e-02, 3.0308e-01, 1.0935e-05,\n",
              "             1.0981e-02, 8.3308e-02]]),\n",
              "    'step': tensor(200.)},\n",
              "   5: {'exp_avg': tensor([-1.0000]),\n",
              "    'exp_avg_sq': tensor([0.8660]),\n",
              "    'step': tensor(200.)}}},\n",
              " <bound method Module.state_dict of MLP(\n",
              "   (layers): Sequential(\n",
              "     (0): Linear(in_features=13, out_features=64, bias=True)\n",
              "     (1): ReLU()\n",
              "     (2): Linear(in_features=64, out_features=32, bias=True)\n",
              "     (3): ReLU()\n",
              "     (4): Linear(in_features=32, out_features=1, bias=True)\n",
              "   )\n",
              " )>)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(validloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltkVXA4qZH18",
        "outputId": "de9ae099-51f1-428c-97e7-c2b5ab550fc8"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[22.057768214832652]"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  \n",
        "  # Load Boston dataset\n",
        "  X, y = load_boston(return_X_y=True)\n",
        "  \n",
        "  # Prepare Boston dataset\n",
        "  dataset = BostonDataset(X, y)\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "  \n",
        "  # Initialize the MLP\n",
        "  mlp = MLP()\n",
        "  \n",
        "  # Define the loss function and optimizer\n",
        "  loss_function = nn.L1Loss()\n",
        "  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "  \n",
        "  # Run the training loop\n",
        "  for epoch in range(0, 5): # 5 epochs at maximum\n",
        "    \n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    \n",
        "    # Set current loss value\n",
        "    current_loss = 0.0\n",
        "    \n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      \n",
        "      # Get and prepare inputs\n",
        "      inputs, targets = data\n",
        "      inputs, targets = inputs.float(), targets.float()\n",
        "      targets = targets.reshape((targets.shape[0], 1))\n",
        "      \n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # Perform forward pass\n",
        "      outputs = mlp(inputs)\n",
        "      \n",
        "      # Compute loss\n",
        "      loss = loss_function(outputs, targets)\n",
        "      \n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "      \n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "      \n",
        "      # Print statistics\n",
        "      current_loss += loss.item()\n",
        "      if i % 10 == 0:\n",
        "          print('Loss after mini-batch %5d: %.3f' %\n",
        "                (i + 1, current_loss / 500))\n",
        "          current_loss = 0.0\n",
        "\n",
        "  # Process is complete.\n",
        "  print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d27f67-03c4-47e4-cf54-b2be75f43a22",
        "id": "ykDpNPYpKGxg"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after mini-batch     1: 0.043\n",
            "Loss after mini-batch    11: 0.426\n",
            "Loss after mini-batch    21: 0.442\n",
            "Loss after mini-batch    31: 0.461\n",
            "Loss after mini-batch    41: 0.452\n",
            "Loss after mini-batch    51: 0.472\n",
            "Starting epoch 2\n",
            "Loss after mini-batch     1: 0.047\n",
            "Loss after mini-batch    11: 0.432\n",
            "Loss after mini-batch    21: 0.450\n",
            "Loss after mini-batch    31: 0.434\n",
            "Loss after mini-batch    41: 0.485\n",
            "Loss after mini-batch    51: 0.434\n",
            "Starting epoch 3\n",
            "Loss after mini-batch     1: 0.049\n",
            "Loss after mini-batch    11: 0.458\n",
            "Loss after mini-batch    21: 0.435\n",
            "Loss after mini-batch    31: 0.423\n",
            "Loss after mini-batch    41: 0.417\n",
            "Loss after mini-batch    51: 0.480\n",
            "Starting epoch 4\n",
            "Loss after mini-batch     1: 0.051\n",
            "Loss after mini-batch    11: 0.436\n",
            "Loss after mini-batch    21: 0.417\n",
            "Loss after mini-batch    31: 0.452\n",
            "Loss after mini-batch    41: 0.447\n",
            "Loss after mini-batch    51: 0.438\n",
            "Starting epoch 5\n",
            "Loss after mini-batch     1: 0.043\n",
            "Loss after mini-batch    11: 0.442\n",
            "Loss after mini-batch    21: 0.427\n",
            "Loss after mini-batch    31: 0.428\n",
            "Loss after mini-batch    41: 0.424\n",
            "Loss after mini-batch    51: 0.451\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l2-v4_zat9Le"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S3b45goht0yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2-3kySLutjdp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}