{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Wrapped up MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8N4y-K7EMeQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libary imports \n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn import decomposition\n",
        "from sklearn import manifold\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "from functools import reduce\n",
        "import operator\n",
        "import torch.utils.data as data_utils\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "metadata": {
        "id": "8aGeFmP-LjRy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "pEgfjtTkLt88"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "\n",
        "# Prepare CIFAR-10 dataset\n",
        "trainset = CIFAR10(os.getcwd(), download=True, transform=transforms.Compose(\n",
        "      [transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
        "\n",
        "testset = CIFAR10(os.getcwd(), download=True, transform=transforms.Compose(\n",
        "      [transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
        "\n",
        "\n",
        "#print(images, labels)\n",
        "\n",
        "def my_fun(dataset):\n",
        "    X=[]\n",
        "    y=[]\n",
        "    for feature, label in iter(dataset):\n",
        "        X.append(feature)\n",
        "        y.append(label)\n",
        "    return X, y\n",
        "\n",
        "   \n",
        "X_train, y_train = my_fun(trainset)\n",
        "X_test, y_test =my_fun(testset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KCxWWGde6bl",
        "outputId": "d0e646b8-2f73-412f-d5db-96de5d180ed5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MLPClassifer(nn.Module):\n",
        "      '''\n",
        "        Multilayer Perceptron.\n",
        "      '''\n",
        "      def __init__(self ):\n",
        "        super().__init__()\n",
        "        input=trainset.data[1].shape\n",
        "        output=len(torch.unique(torch.tensor(trainset.targets)))\n",
        "        hidden_layer_sizes=(64,32)\n",
        "        self.layers = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(reduce(operator.mul, input),  hidden_layer_sizes[0]),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_layer_sizes[0],  hidden_layer_sizes[1]),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_layer_sizes[1], output)\n",
        "        )\n",
        "\n",
        "      def forward(self, x):\n",
        "        '''Forward pass'''\n",
        "        return self.layers(x)\n",
        "\n",
        "class Neural_Network(object):\n",
        "    \n",
        "    \n",
        "    def __init__ (self,batch_size=2000,epochs=2,verbose=True,loss_function = nn.CrossEntropyLoss(),\n",
        "         optimiser='adam',classifier= MLPClassifer(), \n",
        "         learning_rate=1e-3,  betas=(0.9,0.99) , eps=1e-8, weight_decay=0, amsgrad=False,\n",
        "      momentum=0.9, nesterov=True, dampening=0, maximise=False):\n",
        "      \n",
        "      self.batch_size=batch_size\n",
        "      self.epochs=epochs\n",
        "      \n",
        "      self.verbose=True\n",
        "      #used inside the training/ testing loops\n",
        "      self.running_loss = 0\n",
        "      self.loss_function = loss_function\n",
        "      self.classifier= classifier\n",
        "      \n",
        "      self.sm = torch.nn.Softmax()\n",
        "\n",
        "      #some initial parameters for either standard optimisers\n",
        "      self.learning_rate=learning_rate \n",
        "      self.betas= betas\n",
        "      self.eps= eps\n",
        "      self.weight_decay=weight_decay\n",
        "      self.amsgrad=amsgrad \n",
        "      self.momentum=momentum\n",
        "      self.nesterov=nesterov\n",
        "      self.dampening=dampening\n",
        "      self.maximise=maximise\n",
        "\n",
        "      #default optimiser: adam \n",
        "      self.optimiser=self.set_optimiser_2(optimiser)\n",
        "    \n",
        "    #for some reason calling this function set_optimiser returns None \n",
        "    def set_optimiser_2(self, optimiser):\n",
        "        #playing around with adam and sgd only at the moment\n",
        "        #there is only two of them \n",
        "        if optimiser.lower().strip()=='adam':\n",
        "            self.optimiser=torch.optim.Adam(self.classifier.parameters(),lr=self.learning_rate, betas=self.betas, eps=self.eps, weight_decay=self.weight_decay, amsgrad=self.amsgrad)\n",
        "        elif optimiser.lower().strip()=='sgd':\n",
        "            self.optimiser=torch.optim.SGD(self.classifier.parameters(),lr=self.learning_rate, weight_decay=self.weight_decay, momentum=0.9,nesterov=self.nesterov , dampening=self.dampening)\n",
        "        return self.optimiser\n",
        "\n",
        "\n",
        "    def get_optimiser(self):\n",
        "      print(self.optimiser)\n",
        "\n",
        "    \n",
        "    def get_params(self): #get parameters\n",
        "       if (self.optimiser != None) :\n",
        "         return  (self.props(),self.optimiser.state_dict(), self.classifier.state_dict)\n",
        "       else: return self.classifier.state_dict() \n",
        "  \n",
        "    def fit(self,X,y):\n",
        "        train = [*zip(X,y)]\n",
        "        trainloader =  torch.utils.data.DataLoader(train,batch_size=8, shuffle=False, num_workers=2)\n",
        "        for epoch in range(self.epochs):  # loop over the dataset 2 times\n",
        "        \n",
        "          running_loss =self.running_loss\n",
        "          for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "             \n",
        "            # zero the parameter gradients\n",
        "            self.optimiser.zero_grad()\n",
        "\n",
        "            # forward + compute loss+ backward + optimize\n",
        "            outputs = self.classifier(inputs)\n",
        "            loss = self.loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimiser.step()\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % self.batch_size == 1999:    # print every 2000 mini-batches\n",
        "                if self.verbose==True:\n",
        "                  print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / self.batch_size:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    def predict(self,X,y): #Predict using the multi-layer perceptron classifier.\n",
        "      test = [*zip(X,y)]\n",
        "      testloader = torch.utils.data.DataLoader(test,batch_size=8, shuffle=False, num_workers=2)\n",
        "      prediction_list=[]\n",
        "      # again no gradients needed\n",
        "      with torch.no_grad():\n",
        "          for data in testloader:\n",
        "              images, labels = data\n",
        "              outputs = self.classifier(images)\n",
        "              estimation, predictions = torch.max(outputs, 1)\n",
        "              prediction_list.append(predictions)\n",
        "      return prediction_list #torch.stack(prediction_list)\n",
        "\n",
        "\n",
        "    def predict_log_proba(self,X,y):  #\tReturn the log of probability estimates.\n",
        "        data = [*zip(X,y)]\n",
        "        loader =  torch.utils.data.DataLoader(data,batch_size=8, shuffle=False, num_workers=2)\n",
        "        y_prob=self.predict_proba(loader)\n",
        "        log_proba=np.log(y_prob)\n",
        "        return log_proba\n",
        " \n",
        "\n",
        "    def predict_proba(self,X,y):\t#Probability estimates.\n",
        "      probabilities_list=[]\n",
        "      data = [*zip(X,y)]\n",
        "      loader =  torch.utils.data.DataLoader(data,batch_size=8, shuffle=False, num_workers=2)\n",
        "      # again no gradients needed\n",
        "      with torch.no_grad():\n",
        "          for data in loader:\n",
        "              images, labels = data\n",
        "              outputs = self.classifier(images)\n",
        "              probabilities_list.append(self.sm(outputs) )\n",
        "              estimation, predictions = torch.max(outputs, 1)\n",
        "      return probabilities_list\n",
        "      \n",
        "\n",
        "    def score(self,X,y): #Return the mean accuracy on the given test data and labels.\n",
        "        prediction_list=[]\n",
        "        targets_list=[]\n",
        "        data = [*zip(X,y)]\n",
        "        loader =  torch.utils.data.DataLoader(data,batch_size=8, shuffle=False, num_workers=2)\n",
        "        num_classes=len(torch.unique(torch.tensor(y)))\n",
        "        #set it to true if you have a list of available labels in text format\n",
        "        labels_available=False\n",
        "        if labels_available:\n",
        "            classes = ('plane', 'car', 'bird', 'cat',\n",
        "                    'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "            # prepare to count predictions for each class\n",
        "            correct_pred = {classname: 0 for classname in classes}\n",
        "            total_pred = {classname: 0 for classname in classes}\n",
        "        #use the class number as key\n",
        "        else: \n",
        "            classes=tuple(np.arange(num_classes))  \n",
        "            correct_pred = {classname: 0 for classname in np.arange(num_classes)}\n",
        "            total_pred= {classname: 0 for classname in np.arange(num_classes)}\n",
        "\n",
        "        # again no gradients needed\n",
        "        with torch.no_grad():\n",
        "            for data in loader:\n",
        "                images, labels = data\n",
        "                targets_list.append(labels)\n",
        "                outputs = self.classifier(images)\n",
        "                estimation, predictions = torch.max(outputs, 1)\n",
        "                prediction_list.append(predictions)\n",
        "                # collect the correct predictions for each class\n",
        "                for label, prediction in zip(labels, predictions):\n",
        "                    if label == prediction:\n",
        "                        correct_pred[classes[label]] += 1\n",
        "                    total_pred[classes[label]] += 1\n",
        "        accuracies={}\n",
        "        # print accuracy for each class\n",
        "        for classname, correct_count in correct_pred.items():\n",
        "            accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "            accuracies[classname]=accuracy\n",
        "            if self.verbose:\n",
        "              print(f'Accuracy for class: {classname} is {accuracy:.1f} %')\n",
        "        return targets_list,prediction_list\n",
        "        \n",
        "\n",
        "    def set_params(self, attr, value): #sets parameters\n",
        "        setattr(self, attr, value)\n",
        "        \n",
        "    def props(cls):   \n",
        "        return [i for i in cls.__dict__.items() if i[:1] != '_']\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.classifier()\n",
        "\n",
        "    def set_optimiser(self, optimiser_choice):\n",
        "        return \n",
        "\n",
        "    def get_classification_report(y, y_pred):\n",
        "        y_prime = np.stack([np.stack([d for d in d_]) for d_ in y]).flatten()\n",
        "        y_prime=np.stack([np.stack([d for d in d_]) for d_ in y_pred]).flatten()   \n",
        "        print(classification_report(y_prime, y_test))\n",
        "    \n"
      ],
      "metadata": {
        "id": "07OsmxL4G_3_"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= Neural_Network()\n",
        "model.set_optimiser_2('adam')\n",
        "model.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0GPKQoTtZfN",
        "outputId": "32046f12-f642-42ec-9591-afd9f9eeb012"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 1.795\n",
            "[1,  4000] loss: 1.652\n",
            "[1,  6000] loss: 1.626\n",
            "[2,  2000] loss: 1.562\n",
            "[2,  4000] loss: 1.515\n",
            "[2,  6000] loss: 1.522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y, y_pred=model.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzElDmQ4gcmh",
        "outputId": "f5e2481b-de90-400c-d610-c76d6a24609b"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class: 0 is 46.0 %\n",
            "Accuracy for class: 1 is 64.5 %\n",
            "Accuracy for class: 2 is 22.9 %\n",
            "Accuracy for class: 3 is 42.5 %\n",
            "Accuracy for class: 4 is 32.8 %\n",
            "Accuracy for class: 5 is 30.7 %\n",
            "Accuracy for class: 6 is 63.3 %\n",
            "Accuracy for class: 7 is 62.6 %\n",
            "Accuracy for class: 8 is 70.8 %\n",
            "Accuracy for class: 9 is 56.3 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_prime, y_pred_prime))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fTtlnO9gCfe",
        "outputId": "23e90263-e64a-4e3c-df52-46ea84c474de"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.46      0.53      5000\n",
            "           1       0.59      0.65      0.61      5000\n",
            "           2       0.44      0.23      0.30      5000\n",
            "           3       0.32      0.42      0.36      5000\n",
            "           4       0.47      0.33      0.39      5000\n",
            "           5       0.44      0.31      0.36      5000\n",
            "           6       0.45      0.63      0.52      5000\n",
            "           7       0.52      0.63      0.57      5000\n",
            "           8       0.57      0.71      0.63      5000\n",
            "           9       0.54      0.56      0.55      5000\n",
            "\n",
            "    accuracy                           0.49     50000\n",
            "   macro avg       0.49      0.49      0.48     50000\n",
            "weighted avg       0.49      0.49      0.48     50000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "90zQDp31f7Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l2-v4_zat9Le"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}