{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Wrapped up MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP5GbBOBCcptfi75hUXZLtA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cosmina98/PhD/blob/main/MLP_Wrapped_update/Wrapped_up_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "S8pcKI_aR0u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87cF98Fd9r0g",
        "outputId": "2bdd62cb-a922-4603-d733-0bbf4ae92d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30 kB 35.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 51 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 61 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 71 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 81 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 92 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 102 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 112 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 122 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 133 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 143 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 153 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 163 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 174 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 184 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 194 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 204 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 215 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 225 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 235 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 245 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 256 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 266 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 276 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 286 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 296 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 307 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 317 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 327 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 337 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 348 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 358 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 368 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 378 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 389 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 399 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 409 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 419 kB 8.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -U scikit-learn --user\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QWAnRuBxzRw",
        "outputId": "1d950f16-d23d-42ea-833e-dd69ead0159d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre --extra-index https://pypi.anaconda.org/scipy-wheels-nightly/simple scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgOtY6Z5yLv_",
        "outputId": "1fb52e47-74f5-4040-ae02-60d3f341e39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.anaconda.org/scipy-wheels-nightly/simple\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# libary imports \n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn import decomposition\n",
        "from sklearn import manifold\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "\n",
        "import itertools\n",
        "from functools import reduce\n",
        "import operator\n",
        "import torch.utils.data as data_utils\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "from torch.optim.adadelta import Adadelta\n",
        "from torch.optim.adagrad import Adagrad\n",
        "from torch.optim.adam import Adam\n",
        "from torch.optim.adamax import Adamax\n",
        "from torch.optim.adamw import AdamW\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "\n",
        "\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 20]"
      ],
      "metadata": {
        "id": "8aGeFmP-LjRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "pEgfjtTkLt88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main program"
      ],
      "metadata": {
        "id": "OXrJf1YLR3t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classification_report(y, y_pred):\n",
        "        y_prime = np.stack([np.stack([d for d in d_]) for d_ in y]).flatten()\n",
        "        y_pred_prime=np.stack([np.stack([d for d in d_]) for d_ in y_pred]).flatten()   \n",
        "        print(classification_report(y_prime, y_pred_prime))\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "def get_confusion_matrix(y, y_pred, plot=True):\n",
        "      y_prime = np.stack([np.stack([d for d in d_]) for d_ in y]).flatten()\n",
        "      y_pred_prime=np.stack([np.stack([d for d in d_]) for d_ in y_pred]).flatten()\n",
        "      cnf_matrix = confusion_matrix(y_prime, y_pred_prime,labels=np.arange(10))\n",
        "      np.set_printoptions(precision=2)\n",
        "      # Plot non-normalized confusion matrix\n",
        "      if plot==True:\n",
        "        plt.figure()\n",
        "        plot_confusion_matrix(cnf_matrix, classes=['plane', 'car', 'bird', 'cat',\n",
        "                            'deer', 'dog', 'frog', 'horse', 'ship', 'truck'],\n",
        "                              title='Confusion matrix, without normalization')\n",
        "          "
      ],
      "metadata": {
        "id": "e4z0eokLkCWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Prepare CIFAR-10 dataset\n",
        "trainset = CIFAR10(os.getcwd(), download=True, transform=transforms.Compose(\n",
        "      [transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
        "\n",
        "testset = CIFAR10(os.getcwd(), download=True, transform=transforms.Compose(\n",
        "      [transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
        "\n",
        "\n",
        "#print(images, labels)\n",
        "\n",
        "def my_fun(dataset):\n",
        "    X=[]\n",
        "    y=[]\n",
        "    for feature, label in iter(dataset):\n",
        "        X.append(feature)\n",
        "        y.append(label)\n",
        "    return torch.stack(X).numpy(), y\n",
        "\n",
        "\n",
        "#get  data as numpy arrays\n",
        "X_train, y_train = my_fun(trainset)\n",
        "X_test, y_test =my_fun(testset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KCxWWGde6bl",
        "outputId": "f3d6e0a5-4ca9-422c-a70b-60ea8e017e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPClassifer(nn.Module):\n",
        "      '''\n",
        "        Multilayer Perceptron.\n",
        "      '''\n",
        "      def __init__(\n",
        "        self,\n",
        "        input_dim=trainset.data[1].shape,\n",
        "        output_dim=len(torch.unique(torch.tensor(trainset.targets))),\n",
        "        first_hidden_dimension=64,\n",
        "        #if you only want to add  a single hidden layer you do the following:\n",
        "       # hidden_layers=((32),)\n",
        "        hidden_layers=((32),),\n",
        "        ):\n",
        "        super().__init__()\n",
        "        #self.modules1 = torch.nn.ModuleList([])\n",
        "        self.residual=False\n",
        "        self.hidden_layer_num=len(hidden_layers)\n",
        "        self.d_in=reduce(operator.mul, input_dim)\n",
        "        if self.hidden_layer_num>0:\n",
        "            self.h_0=first_hidden_dimension\n",
        "            self.h_n=hidden_layers[-1]\n",
        " \n",
        "        else: \n",
        "            self.h_0=self.h_n=first_hidden_dimension\n",
        "      \n",
        "\n",
        "        self.linear1=nn.Linear(self.d_in,  self.h_0)\n",
        "        self.relu=torch.nn.ReLU()\n",
        "        self.hidden_linear=[]\n",
        "        if self.hidden_layer_num>0:\n",
        "          in_channels=self.h_0\n",
        "          for h_dim in list(hidden_layers):\n",
        "              #self.modules1.append(nn.Linear(in_channels, in_channels))\n",
        "              self.hidden_linear.append(torch.nn.Linear(in_channels, h_dim))\n",
        "              in_channels = h_dim\n",
        "        self.hidden_linear=nn.ModuleList(self.hidden_linear)\n",
        "        self.linear2=nn.Linear( self.h_n, output_dim)\n",
        "        #self.modules1.append(nn.Linear( self.h_n, output_dim))\n",
        "     \n",
        "\n",
        "      def forward(self, x):\n",
        "        x=x.view(-1,self.d_in)\n",
        "        x=self.linear1(x)\n",
        "        x=self.relu(x)\n",
        "        if self.hidden_layer_num>0:\n",
        "          for linear in self.hidden_linear:\n",
        "            if self.residual:\n",
        "              x=self.relu(linear(x)) + x\n",
        "          else:\n",
        "              x = self.relu(linear(x))\n",
        "        return  F.log_softmax(self.linear2(x), dim=1)\n",
        "\n",
        "class Optimiser(object):\n",
        "        def __init__(self,name='Adam',classifier=MLPClassifer(),learning_rate=1e-3,  betas=(0.9,0.99) , eps=1e-8, weight_decay=0, amsgrad=False,\n",
        "      momentum=0.9, nesterov=True, dampening=0, maximize=False,\n",
        "      rho=0.9,lr_decay=0,initial_accumulator_value=0,capturable=False,\n",
        "       max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None,\n",
        "       lambd=0.0001, alpha=0.75, t0=1000000.0, momentum_decay=0.004,centered=False,\n",
        "       etas=(0.5, 1.2), step_sizes=(1e-06, 50)):\n",
        "          self.learning_rate=learning_rate \n",
        "          self.betas= betas\n",
        "          self.eps= eps\n",
        "          self.weight_decay=weight_decay\n",
        "          self.amsgrad=amsgrad \n",
        "          self.momentum=momentum\n",
        "          self.nesterov=nesterov\n",
        "          self.dampening=dampening\n",
        "          self.maximize=maximize\n",
        "          self.classifier=classifier\n",
        "          self.name=name\n",
        "          self.optimiser=self.set_optimizer(self.name)\n",
        "          self.rho=rho\n",
        "          self.lr_decay=lr_decay\n",
        "          self.capturable=capturable\n",
        "\n",
        "          #lbfgs\n",
        "          self.max_iter=max_iter\n",
        "          self.max_eval=max_eval\n",
        "          self.tolerance_grad=tolerance_grad\n",
        "          self.tolerance_change=tolerance_change\n",
        "          self.history_size=history_size\n",
        "          self.line_search_fn=line_search_fn\n",
        "\n",
        "          #asgd\n",
        "          self.lambd=lambd\n",
        "          self.alpha=alpha\n",
        "          self.t0=t0\n",
        "          \n",
        "          #Nadam\n",
        "          self.momentum_decay=momentum_decay\n",
        "\n",
        "          self.centered=centered\n",
        "          self.etas=etas\n",
        "          self.step_sizes=step_sizes\n",
        "\n",
        "        def set_optimizer(self, name):\n",
        "            if name.lower().strip()=='adam':\n",
        "                        self.optimiser=torch.optim.Adam(self.classifier.parameters(),lr=self.learning_rate, betas=self.betas, eps=self.eps, weight_decay=self.weight_decay, amsgrad=self.amsgrad,maximize=self.maximize)\n",
        "            elif name.lower().strip()=='sgd':\n",
        "                        self.optimiser=torch.optim.SGD(self.classifier.parameters(),lr=self.learning_rate, weight_decay=self.weight_decay, momentum=0.9,nesterov=self.nesterov , dampening=self.dampening,maximize=self.maximize)\n",
        "            elif name.lower().strip()=='adadelta':\n",
        "                        self.optimiser=torch.optim.Adadelta(self.classifier.parameters(),lr=self.learning_rate, weight_decay=self.weight_decay, eps=self.eps, rho=self.rho,maximize=self.maximize)\n",
        "            elif name.lower().strip()=='adagrad':\n",
        "                        self.optimiser=torch.optim.Adagrad(self.classifier.parameters(),lr=self.learning_rate, eps=self.eps, weight_decay=self.weight_decay, maximize=self.maximize,lr_decay=self.lr_decay, initial_accumulator_value=self.initial_accumulator_value)\n",
        "            elif name.lower().strip()=='adamw':\n",
        "                        self.optimiser=torch.optim.AdamW(self.classifier.parameters(),lr=self.learning_rate, betas=self.betas, eps=self.eps, weight_decay=self.weight_decay, amsgrad=self.amsgrad,maximize=self.maximize, capturable=self.capturable)\n",
        "            elif name.lower().strip()=='adamax':\n",
        "                        self.optimiser=torch.optim.Adamax(self.classifier.parameters(),lr=self.learning_rate, betas=self.betas, eps=self.eps, weight_decay=self.weight_decay,maximize=self.maximize)\n",
        "            elif name.lower().strip()=='lbfgs':\n",
        "                        self.optimiser=torch.optim.LBFGS(self.classifier.parameters(),lr=self.learning_rate, max_iter=self.max_iter,max_eval=self.max_eval, tolerance_grad=self.tolerance_grad,tolerance_change=self.tolerance_change, history_size=self.history_size,line_search_fn=self.line_search_fn)\n",
        "            elif name.lower().strip()=='asgd':\n",
        "                        self.optimiser=torch.optim.ASGD(self.classifier.parameters(),lr=self.learning_rate, weight_decay=self.weight_decay, lambd=self.lambd, alpha=self.alpha, t0=self.t0 )\n",
        "            elif name.lower().strip()=='nadam':\n",
        "                        self.optimiser=torch.optim.NAdam(self.classifier.parameters(),lr=self.learning_rate, weight_decay=self.weight_decay, eps=self.eps, betas=self.betas, momentum_decay=self.momentum_decay)\n",
        "            elif name.lower().strip()=='radam':\n",
        "                        self.optimiser=torch.optim.RAdam(self.classifier.parameters(),lr=self.learning_rate, weight_decay=self.weight_decay, eps=self.eps, betas=self.betas )\n",
        "            elif name.lower().strip()=='rmsprop':\n",
        "                        self.optimiser=torch.optim.RMSprop(self.classifier.parameters(),lr=self.learning_rate, weight_decay=self.weight_decay, momentum=self.momentum,alpha=self.alpha, eps=self.eps, centered=self.centered)\n",
        "            elif name.lower().strip()=='rprop':\n",
        "                        self.optimiser=torch.optim.Rprop(self.classifier.parameters(),lr=self.learning_rate, etas=self.etas, step_sizes=self.step_sizes)\n",
        "            \n",
        "            return self.optimiser\n",
        "\n",
        "\n",
        "class Neural_Network(object):\n",
        "    \n",
        "    \n",
        "    def __init__ (self,batch_size=2000,epochs=2,verbose=True,loss_function = nn.CrossEntropyLoss(),\n",
        "         optimiser_name='adam',classifier= MLPClassifer()):\n",
        "      \n",
        "      self.batch_size=batch_size\n",
        "      self.epochs=epochs\n",
        "      \n",
        "      self.verbose=True\n",
        "      #used inside the training/ testing loops\n",
        "      self.running_loss = 0\n",
        "      self.loss_function = loss_function\n",
        "      self.classifier= classifier\n",
        "      \n",
        "      self.sm = torch.nn.Softmax()\n",
        "\n",
        "      #default optimiser: adam \n",
        "      self.optimiser=Optimiser(optimiser_name,classifier=self.classifier).optimiser\n",
        "    \n",
        "    \n",
        "    def get_params(self): #get parameters\n",
        "       if (self.optimiser != None) :\n",
        "         return  (self.props(),self.optimiser.state_dict(), self.classifier.state_dict)\n",
        "       else: return self.classifier.state_dict() \n",
        "  \n",
        "    def fit(self,X,y):\n",
        "        train = [*zip(X,y)]\n",
        "        trainloader =  torch.utils.data.DataLoader(train,batch_size=8, shuffle=False, num_workers=2)\n",
        "        for epoch in range(self.epochs):  # loop over the dataset 2 times\n",
        "        \n",
        "          running_loss =self.running_loss\n",
        "          for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "             \n",
        "            # zero the parameter gradients\n",
        "            self.optimiser.zero_grad()\n",
        "\n",
        "            # forward + compute loss+ backward + optimize\n",
        "            outputs = self.classifier(inputs)\n",
        "            loss = self.loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimiser.step()\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % self.batch_size == (self.batch_size-1):    # print every \"batch_size\" mini-batches\n",
        "                if self.verbose==True:\n",
        "                  print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / self.batch_size:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    def predict(self, X_test): #Predict using the multi-layer perceptron classifier.\n",
        "      test = [*zip(X_test,y_test)]\n",
        "      testloader = torch.utils.data.DataLoader(test, shuffle=False)\n",
        "      prediction_list=[]\n",
        "      # again no gradients needed\n",
        "      with torch.no_grad():\n",
        "          for data in testloader:\n",
        "              images, labels = data\n",
        "              outputs = self.classifier(images)\n",
        "              estimation, predictions = torch.max(outputs, 1)\n",
        "              prediction_list.append(predictions)\n",
        "      return prediction_list #torch.stack(prediction_list)\n",
        "\n",
        "\n",
        "    def predict_log_proba(self,X):  #\tReturn the log of probability estimates.\n",
        "        data = [*zip(X,y_test)]\n",
        "        loader =  torch.utils.data.DataLoader(data)\n",
        "        y_prob=self.predict_proba(loader)\n",
        "        log_proba=np.log(y_prob)\n",
        "        return log_proba\n",
        " \n",
        "\n",
        "    def predict_proba(self,X_test):\t#Probability estimates.\n",
        "      probabilities_list=[]\n",
        "      data = [*zip(X_test,y_test)]\n",
        "      loader =  torch.utils.data.DataLoader(data, shuffle=False)\n",
        "      # again no gradients needed\n",
        "      with torch.no_grad():\n",
        "          for data in loader:\n",
        "              images, labels = data\n",
        "              outputs = self.classifier(images)\n",
        "              probabilities_list.append(self.sm(outputs) )\n",
        "              estimation, predictions = torch.max(outputs, 1)\n",
        "      return probabilities_list\n",
        "      \n",
        "\n",
        "    def score(self,X,y): #Return the mean accuracy on the given test data and labels.\n",
        "        prediction_list=[]\n",
        "        targets_list=[]\n",
        "        data = [*zip(X,y)]\n",
        "        loader =  torch.utils.data.DataLoader(data,batch_size=8, shuffle=False, num_workers=2)\n",
        "        num_classes=len(torch.unique(torch.tensor(y)))\n",
        "        #set it to true if you have a list of available labels in text format\n",
        "        labels_available=False\n",
        "        if labels_available:\n",
        "            classes = ('plane', 'car', 'bird', 'cat',\n",
        "                    'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "            # prepare to count predictions for each class\n",
        "            correct_pred = {classname: 0 for classname in classes}\n",
        "            total_pred = {classname: 0 for classname in classes}\n",
        "        #use the class number as key\n",
        "        else: \n",
        "            classes=tuple(np.arange(num_classes))  \n",
        "            correct_pred = {classname: 0 for classname in np.arange(num_classes)}\n",
        "            total_pred= {classname: 0 for classname in np.arange(num_classes)}\n",
        "\n",
        "        # again no gradients needed\n",
        "        with torch.no_grad():\n",
        "            for data in loader:\n",
        "                images, labels = data\n",
        "                targets_list.append(labels)\n",
        "                outputs = self.classifier(images)\n",
        "                estimation, predictions = torch.max(outputs, 1)\n",
        "                prediction_list.append(predictions)\n",
        "                # collect the correct predictions for each class\n",
        "                for label, prediction in zip(labels, predictions):\n",
        "                    if label == prediction:\n",
        "                        correct_pred[classes[label]] += 1\n",
        "                    total_pred[classes[label]] += 1\n",
        "        accuracies={}\n",
        "        # print accuracy for each class\n",
        "        for classname, correct_count in correct_pred.items():\n",
        "            accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "            accuracies[classname]=accuracy\n",
        "            if self.verbose:\n",
        "              print(f'Accuracy for class: {classname} is {accuracy:.1f} %')\n",
        "        return targets_list,prediction_list\n",
        "        \n",
        "\n",
        "    def set_params(self, attr, value): #sets parameters\n",
        "        setattr(self, attr, value)\n",
        "        \n",
        "    def props(cls):   \n",
        "        return [i for i in cls.__dict__.items() if i[:1] != '_']\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.classifier()\n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "07OsmxL4G_3_"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot classifier comparaison\n",
        "\n"
      ],
      "metadata": {
        "id": "4fEokx6at9kN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test_prime, y_pred)\n",
        "\n",
        "cm_display = ConfusionMatrixDisplay(cm).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "wYyb_DBMss1z",
        "outputId": "52dd41a3-0fbe-4ba5-d3f5-5b8170323c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ee608b285936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcm_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "N5YS3TJEt87m",
        "outputId": "a96eb256-66a8-44a0-bb67-4cd02cc1fd33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-a5f0f0e57361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRocCurveDisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, owner)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# delegate only on instances, not the classes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# this is to allow access to the docstrings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mattr_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# raise original `AttributeError` if `attr` does not exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Neural_Network' object has no attribute 'decision_function'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ucGyaNj5u4wE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}