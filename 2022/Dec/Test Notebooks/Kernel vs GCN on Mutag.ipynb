{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9875682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "current = os.getcwd()\n",
    "parent = os.path.dirname(current)\n",
    "sys.path.append(parent)\n",
    "from Kernels.Weisfeiler_Lehman import Weisfeiler_Lehman\n",
    "import torch\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "print(torch.__version__)\n",
    "import torchnet as tnt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import wget\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy.linalg import inv,multi_dot\n",
    "from scipy.linalg import expm\n",
    "from matplotlib import pyplot as plt\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "import numpy as np\n",
    "import pydot\n",
    "from Datasets.utils_mutag import load_data , create_loaders \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Datasets/MUTAG/MUTAG_graph_indicator.txt\n",
      "Data are ready\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data(path='Datasets/MUTAG/', ds_name='MUTAG',\n",
    "                    use_node_labels=True, use_edge_labels=True, max_node_label=7,max_edge_label=4)\n",
    "\n",
    "\n",
    "#pytorch loaders\n",
    "train_dataset, val_dataset = create_loaders(\n",
    "    dataset, batch_size=1, split_id=150, offset=0)\n",
    "print('Data are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f844758",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(dataset,dtype=object)[:,0]\n",
    "y=np.array(dataset,dtype=object)[:,1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa096b",
   "metadata": {},
   "source": [
    "# Testing with Weisfeler Lehman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aaa7b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.16%\n"
     ]
    }
   ],
   "source": [
    "#transform featurex\n",
    "kernel=Weisfeiler_Lehman(normalise=True,h=0,node_label='attr_dict')\n",
    "k_train=kernel.fit_transform(list(X_train))\n",
    "k_test=kernel.transform(list(X_test))\n",
    "# Uses the SVM classifier to perform classification\n",
    "clf = SVC(kernel='precomputed')\n",
    "clf.fit(np.asarray(k_train), np.ravel(y_train).astype(int))\n",
    "y_pred = clf.predict(np.asarray(k_test))\n",
    "# Computes and prints the classification accuracy\n",
    "acc = accuracy_score(np.array(y_test,dtype=int), np.array(y_pred,dtype=int))\n",
    "print(\"Accuracy:\", str(round(acc*100, 2)) + \"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d25c56d",
   "metadata": {},
   "source": [
    "# Testing with an heterogenous arhitecture(MEWISPool)  suposedly  achieving 96% after 200 epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea458e8b",
   "metadata": {},
   "source": [
    "https://paperswithcode.com/sota/graph-classification-on-mutag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b9185a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc802007",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, enhance=False):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.enhance = enhance\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.fc3 = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
    "\n",
    "        if enhance:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        if self.enhance:\n",
    "            x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        if self.enhance:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        if self.enhance:\n",
    "            x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        if self.enhance:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MEWISPool(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(MEWISPool, self).__init__()\n",
    "\n",
    "        self.gc1 = GINConv(MLP(1, hidden_dim, hidden_dim))\n",
    "        self.gc2 = GINConv(MLP(hidden_dim, hidden_dim, hidden_dim))\n",
    "        self.gc3 = GINConv(MLP(hidden_dim, hidden_dim, 1))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # computing the graph laplacian and adjacency matrix\n",
    "        batch_nodes = batch.size(0)\n",
    "        if edge_index.size(1) != 0:\n",
    "            L_indices, L_values = get_laplacian(edge_index)\n",
    "            L = torch.sparse.FloatTensor(L_indices, L_values, torch.Size([batch_nodes, batch_nodes]))\n",
    "            A = torch.diag(torch.diag(L.to_dense())) - L.to_dense()\n",
    "\n",
    "            # entropy computation\n",
    "            entropies = self.compute_entropy(x, L, A, batch)  # Eq. (8)\n",
    "        else:\n",
    "            A = torch.zeros([batch_nodes, batch_nodes])\n",
    "            norm = torch.norm(x, dim=1).unsqueeze(-1)\n",
    "            entropies = norm / norm\n",
    "\n",
    "        # graph convolution and probability scores\n",
    "        probabilities = self.gc1(entropies, edge_index)\n",
    "        probabilities = self.gc2(probabilities, edge_index)\n",
    "        probabilities = self.gc3(probabilities, edge_index)\n",
    "        probabilities = torch.sigmoid(probabilities)\n",
    "\n",
    "        # conditional expectation; Algorithm 1\n",
    "        gamma = entropies.sum()\n",
    "        loss = self.loss_fn(entropies, probabilities, A, gamma)  # Eq. (9)\n",
    "\n",
    "        mewis = self.conditional_expectation(entropies, probabilities, A, loss, gamma)\n",
    "\n",
    "        # graph reconstruction; Eq. (10)\n",
    "        x_pooled, adj_pooled = self.graph_reconstruction(mewis, x, A)\n",
    "        edge_index_pooled, batch_pooled = self.to_edge_index(adj_pooled, mewis, batch)\n",
    "\n",
    "        return x_pooled, edge_index_pooled, batch_pooled, loss, mewis\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_entropy(x, L, A, batch):\n",
    "        # computing local variations; Eq. (5)\n",
    "        V = x * torch.matmul(L, x) - x * torch.matmul(A, x) + torch.matmul(A, x * x)\n",
    "        V = torch.norm(V, dim=1)\n",
    "\n",
    "        # computing the probability distributions based on the local variations; Eq. (7)\n",
    "        P = torch.cat([torch.softmax(V[batch == i], dim=0) for i in torch.unique(batch)])\n",
    "        P[P == 0.] += 1\n",
    "        # computing the entropies; Eq. (8)\n",
    "        H = -P * torch.log(P)\n",
    "\n",
    "        return H.unsqueeze(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_fn(entropies, probabilities, A, gamma):\n",
    "        term1 = -torch.matmul(entropies.t(), probabilities)[0, 0]\n",
    "\n",
    "        term2 = torch.matmul(torch.matmul(probabilities.t(), A), probabilities).sum()\n",
    "\n",
    "        return gamma + term1 + term2\n",
    "\n",
    "    def conditional_expectation(self, entropies, probabilities, A, threshold, gamma):\n",
    "        sorted_probabilities = torch.sort(probabilities, descending=True, dim=0)\n",
    "\n",
    "        dummy_probabilities = probabilities.detach().clone()\n",
    "        selected = set()\n",
    "        rejected = set()\n",
    "\n",
    "        for i in range(sorted_probabilities.values.size(0)):\n",
    "            node_index = sorted_probabilities.indices[i].item()\n",
    "            neighbors = torch.where(A[node_index] == 1)[0]\n",
    "            if len(neighbors) == 0:\n",
    "                selected.add(node_index)\n",
    "                continue\n",
    "            if node_index not in rejected and node_index not in selected:\n",
    "                s = dummy_probabilities.clone()\n",
    "                s[node_index] = 1\n",
    "                s[neighbors] = 0\n",
    "\n",
    "                loss = self.loss_fn(entropies, s, A, gamma)\n",
    "\n",
    "                if loss <= threshold:\n",
    "                    selected.add(node_index)\n",
    "                    for n in neighbors.tolist():\n",
    "                        rejected.add(n)\n",
    "\n",
    "                    dummy_probabilities[node_index] = 1\n",
    "                    dummy_probabilities[neighbors] = 0\n",
    "\n",
    "        mewis = list(selected)\n",
    "        mewis = sorted(mewis)\n",
    "\n",
    "        return mewis\n",
    "\n",
    "    @staticmethod\n",
    "    def graph_reconstruction(mewis, x, A):\n",
    "        x_pooled = x[mewis]\n",
    "\n",
    "        A2 = torch.matmul(A, A)\n",
    "        A3 = torch.matmul(A2, A)\n",
    "\n",
    "        A2 = A2[mewis][:, mewis]\n",
    "        A3 = A3[mewis][:, mewis]\n",
    "\n",
    "        I = torch.eye(len(mewis))\n",
    "        one = torch.ones([len(mewis), len(mewis)])\n",
    "\n",
    "        adj_pooled = (one - I) * torch.clamp(A2 + A3, min=0, max=1)\n",
    "\n",
    "        return x_pooled, adj_pooled\n",
    "\n",
    "    @staticmethod\n",
    "    def to_edge_index(adj_pooled, mewis, batch):\n",
    "        row1, row2 = torch.where(adj_pooled > 0)\n",
    "        edge_index_pooled = torch.cat([row1.unsqueeze(0), row2.unsqueeze(0)], dim=0)\n",
    "        batch_pooled = batch[mewis]\n",
    "\n",
    "        return edge_index_pooled, batch_pooled\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.gc1 = GINConv(MLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n",
    "        self.pool1 = MEWISPool(hidden_dim=hidden_dim)\n",
    "        self.gc2 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n",
    "        self.pool2 = MEWISPool(hidden_dim=hidden_dim)\n",
    "        self.gc3 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n",
    "        self.fc1 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.gc1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x_pooled1, edge_index_pooled1, batch_pooled1, loss1, mewis = self.pool1(x, edge_index, batch)\n",
    "\n",
    "        x_pooled1 = self.gc2(x_pooled1, edge_index_pooled1)\n",
    "        x_pooled1 = torch.relu(x_pooled1)\n",
    "\n",
    "        x_pooled2, edge_index_pooled2, batch_pooled2, loss2, mewis = self.pool2(x_pooled1, edge_index_pooled1,\n",
    "                                                                                batch_pooled1)\n",
    "\n",
    "        x_pooled2 = self.gc3(x_pooled2, edge_index_pooled2)\n",
    "\n",
    "        readout = torch.cat([x_pooled2[batch_pooled2 == i].mean(0).unsqueeze(0) for i in torch.unique(batch_pooled2)],\n",
    "                            dim=0)\n",
    "\n",
    "        out = self.fc1(readout)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return torch.log_softmax(out, dim=-1), loss1 + loss2\n",
    "\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(Net2, self).__init__()\n",
    "\n",
    "        self.gc1 = GINConv(MLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n",
    "        self.gc2 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n",
    "        self.gc3 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n",
    "        self.pool1 = MEWISPool(hidden_dim=hidden_dim)\n",
    "        self.gc4 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n",
    "        self.fc1 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.gc1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.gc2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.gc3(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        readout2 = torch.cat([x[batch == i].mean(0).unsqueeze(0) for i in torch.unique(batch)], dim=0)\n",
    "\n",
    "        x_pooled1, edge_index_pooled1, batch_pooled1, loss1, mewis = self.pool1(x, edge_index, batch)\n",
    "\n",
    "        x_pooled1 = self.gc4(x_pooled1, edge_index_pooled1)\n",
    "\n",
    "        readout = torch.cat([x_pooled1[batch_pooled1 == i].mean(0).unsqueeze(0) for i in torch.unique(batch_pooled1)],\n",
    "                            dim=0)\n",
    "\n",
    "        out = readout2 + readout\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return torch.log_softmax(out, dim=-1), loss1\n",
    "\n",
    "\n",
    "class Net3(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(Net3, self).__init__()\n",
    "\n",
    "        self.gc1 = GINConv(MLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n",
    "        self.gc2 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n",
    "        self.pool1 = MEWISPool(hidden_dim=hidden_dim)\n",
    "        self.gc3 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n",
    "        self.gc4 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n",
    "        self.pool2 = MEWISPool(hidden_dim=hidden_dim)\n",
    "        self.gc5 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n",
    "        self.fc1 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.gc1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.gc2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x_pooled1, edge_index_pooled1, batch_pooled1, loss1, mewis1 = self.pool1(x, edge_index, batch)\n",
    "\n",
    "        x_pooled1 = self.gc3(x_pooled1, edge_index_pooled1)\n",
    "        x_pooled1 = torch.relu(x_pooled1)\n",
    "\n",
    "        x_pooled1 = self.gc4(x_pooled1, edge_index_pooled1)\n",
    "        x_pooled1 = torch.relu(x_pooled1)\n",
    "\n",
    "        x_pooled2, edge_index_pooled2, batch_pooled2, loss2, mewis2 = self.pool2(x_pooled1, edge_index_pooled1,\n",
    "                                                                                 batch_pooled1)\n",
    "\n",
    "        x_pooled2 = self.gc5(x_pooled2, edge_index_pooled2)\n",
    "        x_pooled2 = torch.relu(x_pooled2)\n",
    "\n",
    "        readout = torch.cat([x_pooled2[batch_pooled2 == i].mean(0).unsqueeze(0) for i in torch.unique(batch_pooled2)],\n",
    "                            dim=0)\n",
    "\n",
    "        out = self.fc1(readout)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return torch.log_softmax(out, dim=-1), loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e63ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "torch.manual_seed(7)\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader \n",
    "from torch_geometric.nn import GINConv\n",
    "from torch_geometric.utils import get_laplacian\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "DATASET_NAME = 'MUTAG'\n",
    "BATCH_SIZE = 20\n",
    "HIDDEN_DIM = 32\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-2\n",
    "WEIGHT_DECAY = 1e-5\n",
    "SCHEDULER_PATIENCE = 10\n",
    "SCHEDULER_FACTOR = 0.1\n",
    "EARLY_STOPPING_PATIENCE = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b3d64168",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = osp.join(osp.dirname(osp.abspath(\"__file__\")), '../Datasets/MutagII', DATASET_NAME)\n",
    "dataset = TUDataset(path, name=DATASET_NAME, use_node_attr=True, use_edge_attr=True).shuffle()\n",
    "n = (len(dataset) + 9) // 10\n",
    "\n",
    "input_dim = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "test_dataset = dataset[:n]\n",
    "val_dataset = dataset[n:2 * n]\n",
    "train_dataset = dataset[2 * n:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "333b818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0, Train Loss: 1.857, Train Acc: 0.64, Val Loss: 1.483, Val Acc: 0.68, Test Loss: 1.398, Test Acc: 0.00, Elapsed Time: 0.9\n",
      "[*] Epoch: 1, Train Loss: 0.654, Train Acc: 0.71, Val Loss: 1.007, Val Acc: 0.84, Test Loss: 0.828, Test Acc: 0.89, Elapsed Time: 1.0\n",
      "[*] Epoch: 2, Train Loss: 0.564, Train Acc: 0.75, Val Loss: 0.690, Val Acc: 0.74, Test Loss: 0.560, Test Acc: 0.95, Elapsed Time: 1.1\n",
      "[*] Epoch: 3, Train Loss: 0.635, Train Acc: 0.69, Val Loss: 0.554, Val Acc: 0.84, Test Loss: 0.241, Test Acc: 0.84, Elapsed Time: 0.9\n",
      "[*] Epoch: 4, Train Loss: 0.510, Train Acc: 0.73, Val Loss: 0.442, Val Acc: 0.84, Test Loss: 0.300, Test Acc: 0.89, Elapsed Time: 0.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model, Optimizer and Loss definitions\n",
    "model = Net3(input_dim=input_dim, hidden_dim=HIDDEN_DIM, num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                       patience=SCHEDULER_PATIENCE,\n",
    "                                                       factor=SCHEDULER_FACTOR,\n",
    "                                                       verbose=True)\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_test_acc = 0\n",
    "wait = None\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training the model\n",
    "    s_time = time.time()\n",
    "    train_loss = 0.\n",
    "    train_corrects = 0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        s = time.time()\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, loss_pool = model(data.x, data.edge_index, data.batch)\n",
    "        loss_classification = nll_loss(out, data.y.view(-1))\n",
    "        loss = loss_classification + 0.01 * loss_pool\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        train_corrects += out.max(dim=1)[1].eq(data.y.view(-1)).sum().item()\n",
    "        optimizer.step()\n",
    "        # print(f'{i}/{len(train_loader)}, {time.time() - s}')\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = train_corrects / len(train_dataset)\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.\n",
    "    val_corrects = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            s = time.time()\n",
    "            data = data.to(device)\n",
    "            out, loss_pool = model(data.x, data.edge_index, data.batch)\n",
    "            loss_classification = nll_loss(out, data.y.view(-1))\n",
    "            loss = loss_classification + 0.01 * loss_pool\n",
    "            val_loss += loss.item()\n",
    "            val_corrects += out.max(dim=1)[1].eq(data.y.view(-1)).sum().item()\n",
    "            # print(f'{i}/{len(val_loader)}, {time.time() - s}')\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = val_corrects / len(val_dataset)\n",
    "\n",
    "    # Test\n",
    "    test_loss = 0.\n",
    "    test_corrects = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            s = time.time()\n",
    "            data = data.to(device)\n",
    "            out, loss_pool = model(data.x, data.edge_index, data.batch)\n",
    "            loss_classification = nll_loss(out, data.y.view(-1))\n",
    "            loss = loss_classification + 0.01 * loss_pool\n",
    "            test_loss += loss.item()\n",
    "            test_corrects += out.max(dim=1)[1].eq(data.y.view(-1)).sum().item()\n",
    "            # print(f'{i}/{len(val_loader)}, {time.time() - s}')\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = test_corrects / len(test_dataset)\n",
    "\n",
    "    elapse_time = time.time() - s_time\n",
    "    log = '[*] Epoch: {}, Train Loss: {:.3f}, Train Acc: {:.2f}, Val Loss: {:.3f}, ' \\\n",
    "          'Val Acc: {:.2f}, Test Loss: {:.3f}, Test Acc: {:.2f}, Elapsed Time: {:.1f}'\\\n",
    "        .format(epoch, train_loss, train_acc, val_loss, val_acc, test_loss, best_test_acc, elapse_time)\n",
    "    print(log)\n",
    "\n",
    "    # Early-Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_test_acc = test_acc\n",
    "        wait = 0\n",
    "        # saving the model with best validation loss\n",
    "        torch.save(model.state_dict(), f'checkpoints/{DATASET_NAME}.pkl')\n",
    "    else:\n",
    "        wait += 1\n",
    "    # early stopping\n",
    "    if wait == EARLY_STOPPING_PATIENCE:\n",
    "        print('======== Early stopping! ========')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "06a8be8316eed2f6558da5cb68a9abde15f0a0ec61139bf81bc916fa7c6839e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
