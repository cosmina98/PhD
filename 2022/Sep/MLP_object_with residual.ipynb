{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMY0wz6qVBr5STMnIyfNdg3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cosmina98/PhD/blob/main/sep/MLP_object_with%20residual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTau3RxbDL8m"
      },
      "outputs": [],
      "source": [
        "class MLPClassifer(nn.Module):\n",
        "      '''\n",
        "        Multilayer Perceptron.\n",
        "      '''\n",
        "      def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        output_dim,\n",
        "        #if you only want to add  a single hidden layer you do the following:\n",
        "        hidden_layers=((64)),\n",
        "        #hidden_layers=(),\n",
        "        ):\n",
        "        super().__init__()\n",
        "        #self.modules1 = torch.nn.ModuleList([])\n",
        "        self.residual=False\n",
        "        self.output_dim=output_dim\n",
        "        first_hidden_dimension=hidden_layers[0]\n",
        "        self.hidden_layer_num=len(hidden_layers)-1\n",
        "        self.d_in=input_dim\n",
        "        if self.hidden_layer_num>0:\n",
        "            self.h_0=first_hidden_dimension\n",
        "            self.h_n=hidden_layers[-1]\n",
        " \n",
        "        else: \n",
        "            self.h_0=self.h_n=first_hidden_dimension\n",
        "      \n",
        "\n",
        "        self.linear1=nn.Linear(self.d_in,  self.h_0)\n",
        "        self.relu=torch.nn.ReLU()\n",
        "        self.hidden_linear=[]\n",
        "        if self.hidden_layer_num>0:\n",
        "          in_channels=self.h_0\n",
        "          for h_dim in list(hidden_layers[1:]):\n",
        "              #self.modules1.append(nn.Linear(in_channels, in_channels))\n",
        "              self.hidden_linear.append(torch.nn.Linear(in_channels, h_dim))\n",
        "              in_channels = h_dim\n",
        "        self.hidden_linear=nn.ModuleList(self.hidden_linear)\n",
        "        self.linear2=nn.Linear( self.h_n,  self.output_dim)\n",
        "        #self.modules1.append(nn.Linear( self.h_n, output_dim))\n",
        "        \n",
        "        \n",
        "        \n",
        "      def forward(self, x):\n",
        "        x=x.view(-1,self.d_in)\n",
        "        x=self.linear1(x)\n",
        "        x=self.relu(x)\n",
        "        if self.hidden_layer_num>0:\n",
        "          for linear in self.hidden_linear:\n",
        "            if self.residual:\n",
        "              x=self.relu(linear(x)) + x\n",
        "          else:\n",
        "              x = self.relu(linear(x))\n",
        "        if self.output_dim==1:\n",
        "           return  F.sigmoid(self.linear2(x))\n",
        "        else: return F.log_softmax(self.linear2(x), dim=1)"
      ]
    }
  ]
}